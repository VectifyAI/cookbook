{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI7vcz7-ooxy"
      },
      "source": [
        "# üìÑ PageIndex OCR SDK: Python Quickstart\n",
        "\n",
        "Welcome to the PageIndex OCR SDK tutorial!\n",
        "\n",
        "This notebook will guide you step-by-step through installing the SDK, authenticating with your API key, uploading a PDF for OCR, checking the processing status, gettting OCR results, and finally cleaning up by deleting your document.\n",
        "\n",
        "> **You‚Äôll need your [API Key](https://dash.pageindex.ai/api-keys) to run this notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIjn2vW1ooxz"
      },
      "source": [
        "## 1Ô∏è‚É£ Install the SDK\n",
        "\n",
        "Run the cell below to install the PageIndex OCR SDK. This only needs to be done once in your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY9af_12oox0",
        "outputId": "67c620b7-e420-48b4-9fd8-948796048d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pageindex in /opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages (0.1.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages (from pageindex) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->pageindex) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->pageindex) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->pageindex) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->pageindex) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install pageindex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mgLrLd6oox0"
      },
      "source": [
        "## 2Ô∏è‚É£ Initialize the Client\n",
        "\n",
        "Import the PageIndex client class and authenticate using your API key. Be sure to keep your API key secret and never share it publicly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr9JN4Tsoox0"
      },
      "outputs": [],
      "source": [
        "from pageindex import PageIndexClient\n",
        "\n",
        "# Paste your API key here, you can get the api key from https://dash.pageindex.ai/api-keys\n",
        "API_KEY = \"YOUR_API_KEY\"  \n",
        "pi_client = PageIndexClient(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWIFjkH2oox0"
      },
      "source": [
        "## 3Ô∏è‚É£ Submit a PDF Document for OCR\n",
        "\n",
        "Use the client to upload a PDF file for OCR processing (currently supports PDF files only).\n",
        "\n",
        "After submission, you'll receive a `doc_id` that you can use to check status and get OCR results.\n",
        "\n",
        "> Replace the file path below with your own PDF file if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "znRzYxHQoox0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded file to: ../data/2501.12948.pdf\n",
            "Document submitted. Document ID: pi-cmdwxsio400000apg8qntv2yh\n"
          ]
        }
      ],
      "source": [
        "import requests, os\n",
        "\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948.pdf\"\n",
        "pdf_path = os.path.join(\"../data\", pdf_url.split('/')[-1])\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "response = requests.get(pdf_url)\n",
        "with open(pdf_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(f\"Downloaded file to: {pdf_path}\")\n",
        "result = pi_client.submit_document(pdf_path)\n",
        "doc_id = result[\"doc_id\"]\n",
        "print(f\"Document submitted. Document ID: {doc_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYyFY2aXoox0"
      },
      "source": [
        "## 4Ô∏è‚É£ Check Status and Get OCR Results\n",
        "\n",
        "OCR processing may take anywhere from a few seconds (for small files) to several minutes (for larger files).\n",
        "\n",
        "This code polls the service every 3 seconds, for up to 5 minutes. Once finished, it previews the extracted text from the first page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "E5NFI1XRoox1",
        "outputId": "c03cbf31-d4c3-4d08-97fb-847587cdb371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OCR Results ready!\n",
            "Page 1 (partial content):\n",
            "\n",
            "# DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n",
            "\n",
            "DeepSeek-AI<br>research@deepseek.com\n",
            "\n",
            "\n",
            "## Abstract\n",
            "\n",
            "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Simple polling\n",
        "for attempt in range(60):  # Try up to 300s (100 x 3s)\n",
        "    ocr_result = pi_client.get_ocr(doc_id)\n",
        "    if ocr_result[\"status\"] == \"completed\":\n",
        "        print(\"OCR Results ready!\")\n",
        "        break\n",
        "    elif ocr_result[\"status\"] == \"failed\":\n",
        "        print(\"OCR failed.\")\n",
        "        break\n",
        "    time.sleep(3)\n",
        "else:\n",
        "    print(\"Still processing after 10 minutes. Try again later.\")\n",
        "\n",
        "# Preview the first page's markdown\n",
        "if ocr_result.get(\"status\") == \"completed\":\n",
        "    if ocr_result[\"result\"]:\n",
        "        first_page = ocr_result[\"result\"][0]\n",
        "        print(f\"Page {first_page['page_index']} (partial content):\\n\")\n",
        "        print(first_page[\"markdown\"][:1000])  # Print first 1000 chars\n",
        "    else:\n",
        "        print(\"No pages found in OCR result.\")\n",
        "else:\n",
        "    print(\"OCR not completed yet. Try again later.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBgnTpNJoox1"
      },
      "source": [
        "## 5Ô∏è‚É£ Get the Document Tree Structure\n",
        "\n",
        "You can also get the document's PageIndex tree structure using the method below. If the tree is not ready yet, try again later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NwR0ooMZoox1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document tree structure loaded!\n",
            "[{'node_id': '0000',\n",
            "  'nodes': [{'node_id': '0001',\n",
            "             'page_index': 1,\n",
            "             'text': '## Abstract\\n'\n",
            "                     '\\n'\n",
            "                     'We introduce our first-generation reasoning models, '\n",
            "                     'DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a '\n",
            "                     'model trained via large-scale reinforcement learning '\n",
            "                     '(RL) without supervised fine-tuning (SFT) as a '\n",
            "                     'preliminary step, demonstrates remarkable reasoning '\n",
            "                     'capabilities. Through RL, DeepSeek-R1-Zero naturally '\n",
            "                     'emerges with numerous powerful and intriguing reasoning '\n",
            "                     'behaviors. However, it encounters challenges such as '\n",
            "                     'poor readability, and language mixing. To address these '\n",
            "                     'issues and further enhance reasoning performance, we '\n",
            "                     'introduce DeepSeek-R1, which incorporates multi-stage '\n",
            "                     'training and cold-start data before RL. DeepSeekR1 '\n",
            "                     'achieves performance comparable to OpenAI-o1-1217 on '\n",
            "                     'reasoning tasks. To support the research community, we '\n",
            "                     'open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense '\n",
            "                     'models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from '\n",
            "                     'DeepSeek-R1 based on Qwen and Llama.\\n'\n",
            "                     '\\n'\n",
            "                     '\\n'\n",
            "                     '![img-0.jpeg](img-0.jpeg)\\n'\n",
            "                     '\\n'\n",
            "                     'Figure 1 | Benchmark performance of DeepSeek-R1.\\n',\n",
            "             'title': 'Abstract'},\n",
            "            {'node_id': '0002',\n",
            "             'page_index': 2,\n",
            "             'text': '## Contents\\n'\n",
            "                     '\\n'\n",
            "                     '1 Introduction ..... 3\\n'\n",
            "                     '1.1 Contributions ..... 4\\n'\n",
            "                     '1.2 Summary of Evaluation Results ..... 4\\n'\n",
            "                     '2 Approach ..... 5\\n'\n",
            "                     '2.1 Overview ..... 5\\n'\n",
            "                     '2.2 DeepSeek-R1-Zero: Reinforcement Learning on the Base '\n",
            "                     'Model ..... 5\\n'\n",
            "                     '2.2.1 Reinforcement Learning Algorithm ..... 5\\n'\n",
            "                     '2.2.2 Reward Modeling ..... 6\\n'\n",
            "                     '2.2.3 Training Template ..... 6\\n'\n",
            "                     '2.2.4 Performance, Self-evolution Process and Aha Moment '\n",
            "                     'of DeepSeek-R1-Zero ..... 6\\n'\n",
            "                     '2.3 DeepSeek-R1: Reinforcement Learning with Cold Start '\n",
            "                     '..... 9\\n'\n",
            "                     '2.3.1 Cold Start ..... 9\\n'\n",
            "                     '2.3.2 Reasoning-oriented Reinforcement Learning ..... '\n",
            "                     '10\\n'\n",
            "                     '2.3.3 Rejection Sampling and Supervised Fine-Tuning '\n",
            "                     '..... 10\\n'\n",
            "                     '2.3.4 Reinforcement Learning for all Scenarios ..... 11\\n'\n",
            "                     '2.4 Distillation: Empower Small Models with Reasoning '\n",
            "                     'Capability ..... 11\\n'\n",
            "                     '3 Experiment ..... 11\\n'\n",
            "                     '3.1 DeepSeek-R1 Evaluation ..... 13\\n'\n",
            "                     '3.2 Distilled Model Evaluation ..... 14\\n'\n",
            "                     '4 Discussion ..... 14\\n'\n",
            "                     '4.1 Distillation v.s. Reinforcement Learning ..... 14\\n'\n",
            "                     '4.2 Unsuccessful Attempts ..... 15\\n'\n",
            "                     '5 Conclusion, Limitations, and Future Work ..... 16\\n'\n",
            "                     'A Contributions and Acknowledgments ..... 20\\n',\n",
            "             'title': 'Contents'},\n",
            "            {'node_id': '0003',\n",
            "             'nodes': [{'node_id': '0004',\n",
            "                        'nodes': [{'node_id': '0005',\n",
            "                                   'page_index': 4,\n",
            "                                   'text': '#### Post-Training: Large-Scale '\n",
            "                                           'Reinforcement Learning on the Base '\n",
            "                                           'Model\\n'\n",
            "                                           '\\n'\n",
            "                                           '- We directly apply RL to the base '\n",
            "                                           'model without relying on '\n",
            "                                           'supervised fine-tuning (SFT) as a '\n",
            "                                           'preliminary step. This approach '\n",
            "                                           'allows the model to explore '\n",
            "                                           'chain-of-thought (CoT) for solving '\n",
            "                                           'complex problems, resulting in the '\n",
            "                                           'development of DeepSeek-R1-Zero. '\n",
            "                                           'DeepSeek-R1-Zero demonstrates '\n",
            "                                           'capabilities such as '\n",
            "                                           'self-verification, reflection, and '\n",
            "                                           'generating long CoTs, marking a '\n",
            "                                           'significant milestone for the '\n",
            "                                           'research community. Notably, it is '\n",
            "                                           'the first open research to '\n",
            "                                           'validate that reasoning '\n",
            "                                           'capabilities of LLMs can be '\n",
            "                                           'incentivized purely through RL, '\n",
            "                                           'without the need for SFT. This '\n",
            "                                           'breakthrough paves the way for '\n",
            "                                           'future advancements in this area.\\n'\n",
            "                                           '- We introduce our pipeline to '\n",
            "                                           'develop DeepSeek-R1. The pipeline '\n",
            "                                           'incorporates two RL stages aimed '\n",
            "                                           'at discovering improved reasoning '\n",
            "                                           'patterns and aligning with human '\n",
            "                                           'preferences, as well as two SFT '\n",
            "                                           'stages that serve as the seed for '\n",
            "                                           \"the model's reasoning and \"\n",
            "                                           'non-reasoning capabilities. We '\n",
            "                                           'believe the pipeline will benefit '\n",
            "                                           'the industry by creating better '\n",
            "                                           'models.\\n',\n",
            "                                   'title': 'Post-Training: Large-Scale '\n",
            "                                            'Reinforcement Learning on the '\n",
            "                                            'Base Model'},\n",
            "                                  {'node_id': '0006',\n",
            "                                   'page_index': 4,\n",
            "                                   'text': '#### Distillation: Smaller Models '\n",
            "                                           'Can Be Powerful Too\\n'\n",
            "                                           '\\n'\n",
            "                                           '- We demonstrate that the '\n",
            "                                           'reasoning patterns of larger '\n",
            "                                           'models can be distilled into '\n",
            "                                           'smaller models, resulting in '\n",
            "                                           'better performance compared to the '\n",
            "                                           'reasoning patterns discovered '\n",
            "                                           'through RL on small models. The '\n",
            "                                           'open source DeepSeek-R1, as well '\n",
            "                                           'as its API, will benefit the '\n",
            "                                           'research community to distill '\n",
            "                                           'better smaller models in the '\n",
            "                                           'future.\\n'\n",
            "                                           '- Using the reasoning data '\n",
            "                                           'generated by DeepSeek-R1, we '\n",
            "                                           'fine-tuned several dense models '\n",
            "                                           'that are widely used in the '\n",
            "                                           'research community. The evaluation '\n",
            "                                           'results demonstrate that the '\n",
            "                                           'distilled smaller dense models '\n",
            "                                           'perform exceptionally well on '\n",
            "                                           'benchmarks. '\n",
            "                                           'DeepSeek-R1-Distill-Qwen-7B '\n",
            "                                           'achieves 55.5\\\\% on AIME 2024, '\n",
            "                                           'surpassing QwQ-32B-Preview. '\n",
            "                                           'Additionally, '\n",
            "                                           'DeepSeek-R1-Distill-Qwen-32B '\n",
            "                                           'scores 72.6\\\\% on AIME 2024, '\n",
            "                                           '94.3\\\\% on MATH-500, and $57.2 '\n",
            "                                           '\\\\%$ on LiveCodeBench. These '\n",
            "                                           'results significantly outperform '\n",
            "                                           'previous opensource models and are '\n",
            "                                           'comparable to o1-mini. We '\n",
            "                                           'open-source distilled 1.5B, 7B, '\n",
            "                                           '8B, 14B, 32B, and 70B checkpoints '\n",
            "                                           'based on Qwen2.5 and Llama3 series '\n",
            "                                           'to the community.\\n',\n",
            "                                   'title': 'Distillation: Smaller Models Can '\n",
            "                                            'Be Powerful Too'}],\n",
            "                        'page_index': 4,\n",
            "                        'text': '### 1.1. Contributions\\n',\n",
            "                        'title': '1.1. Contributions'},\n",
            "                       {'node_id': '0007',\n",
            "                        'page_index': 4,\n",
            "                        'text': '### 1.2. Summary of Evaluation Results\\n'\n",
            "                                '\\n'\n",
            "                                '- Reasoning tasks: (1) DeepSeek-R1 achieves a '\n",
            "                                'score of 79.8\\\\% Pass@1 on AIME 2024, '\n",
            "                                'slightly surpassing OpenAI-o1-1217. On '\n",
            "                                'MATH-500, it attains an impressive score of '\n",
            "                                '$97.3 \\\\%$, performing on par with '\n",
            "                                'OpenAI-o1-1217 and significantly '\n",
            "                                'outperforming other models. (2) On '\n",
            "                                'coding-related tasks, DeepSeek-R1 '\n",
            "                                'demonstrates expert level in code competition '\n",
            "                                'tasks, as it achieves 2,029 Elo rating on '\n",
            "                                'Codeforces outperforming $96.3 \\\\%$ human '\n",
            "                                'participants in the competition. For '\n",
            "                                'engineering-related tasks, DeepSeek-R1 '\n",
            "                                'performs slightly better than DeepSeek-V3, '\n",
            "                                'which could help developers in real world '\n",
            "                                'tasks.\\n'\n",
            "                                '- Knowledge: On benchmarks such as MMLU, '\n",
            "                                'MMLU-Pro, and GPQA Diamond, DeepSeekR1 '\n",
            "                                'achieves outstanding results, significantly '\n",
            "                                'outperforming DeepSeek-V3 with scores of '\n",
            "                                '$90.8 \\\\%$ on MMLU, $84.0 \\\\%$ on MMLU-Pro, '\n",
            "                                'and $71.5 \\\\%$ on GPQA Diamond. While its '\n",
            "                                'performance is slightly below that of '\n",
            "                                'OpenAI-o1-1217 on these benchmarks, '\n",
            "                                'DeepSeek-R1 surpasses other closed-source '\n",
            "                                'models, demonstrating its competitive edge in '\n",
            "                                'educational tasks. On the factual benchmark '\n",
            "                                'SimpleQA, DeepSeek-R1 outperforms '\n",
            "                                'DeepSeek-V3, demonstrating its capability in '\n",
            "                                'handling fact-based queries. A similar trend '\n",
            "                                'is observed where OpenAI-o1 surpasses 40 on '\n",
            "                                'this benchmark.\\n'\n",
            "                                '\\n'\n",
            "                                '- Others: DeepSeek-R1 also excels in a wide '\n",
            "                                'range of tasks, including creative writing, '\n",
            "                                'general question answering, editing, '\n",
            "                                'summarization, and more. It achieves an '\n",
            "                                'impressive length-controlled win-rate of '\n",
            "                                '$87.6 \\\\%$ on AlpacaEval 2.0 and a win-rate '\n",
            "                                'of $92.3 \\\\%$ on ArenaHard, showcasing its '\n",
            "                                'strong ability to intelligently handle '\n",
            "                                'non-exam-oriented queries. Additionally, '\n",
            "                                'DeepSeek-R1 demonstrates outstanding '\n",
            "                                'performance on tasks requiring long-context '\n",
            "                                'understanding, substantially outperforming '\n",
            "                                'DeepSeek-V3 on long-context benchmarks.\\n',\n",
            "                        'title': '1.2. Summary of Evaluation Results'}],\n",
            "             'page_index': 3,\n",
            "             'text': '## 1. Introduction\\n'\n",
            "                     '\\n'\n",
            "                     'In recent years, Large Language Models (LLMs) have been '\n",
            "                     'undergoing rapid iteration and evolution (Anthropic, '\n",
            "                     '2024; Google, 2024; OpenAI, 2024a), progressively '\n",
            "                     'diminishing the gap towards Artificial General '\n",
            "                     'Intelligence (AGI).\\n'\n",
            "                     '\\n'\n",
            "                     'Recently, post-training has emerged as an important '\n",
            "                     'component of the full training pipeline. It has been '\n",
            "                     'shown to enhance accuracy on reasoning tasks, align with '\n",
            "                     'social values, and adapt to user preferences, all while '\n",
            "                     'requiring relatively minimal computational resources '\n",
            "                     'against pre-training. In the context of reasoning '\n",
            "                     \"capabilities, OpenAI's ol (OpenAI, 2024b) series models \"\n",
            "                     'were the first to introduce inference-time scaling by '\n",
            "                     'increasing the length of the Chain-ofThought reasoning '\n",
            "                     'process. This approach has achieved significant '\n",
            "                     'improvements in various reasoning tasks, such as '\n",
            "                     'mathematics, coding, and scientific reasoning. However, '\n",
            "                     'the challenge of effective test-time scaling remains an '\n",
            "                     'open question for the research community. Several prior '\n",
            "                     'works have explored various approaches, including '\n",
            "                     'process-based reward models (Lightman et al., 2023; '\n",
            "                     'Uesato et al., 2022; Wang et al., 2023), reinforcement '\n",
            "                     'learning (Kumar et al., 2024), and search algorithms '\n",
            "                     'such as Monte Carlo Tree Search and Beam Search (Feng et '\n",
            "                     'al., 2024; Trinh et al., 2024; Xin et al., 2024). '\n",
            "                     'However, none of these methods has achieved general '\n",
            "                     \"reasoning performance comparable to OpenAI's ol series \"\n",
            "                     'models.\\n'\n",
            "                     '\\n'\n",
            "                     'In this paper, we take the first step toward improving '\n",
            "                     'language model reasoning capabilities using pure '\n",
            "                     'reinforcement learning (RL). Our goal is to explore the '\n",
            "                     'potential of LLMs to develop reasoning capabilities '\n",
            "                     'without any supervised data, focusing on their '\n",
            "                     'self-evolution through a pure RL process. Specifically, '\n",
            "                     'we use DeepSeek-V3-Base as the base model and employ '\n",
            "                     'GRPO (Shao et al., 2024) as the RL framework to improve '\n",
            "                     'model performance in reasoning. During training, '\n",
            "                     'DeepSeek-R1-Zero naturally emerged with numerous '\n",
            "                     'powerful and interesting reasoning behaviors. After '\n",
            "                     'thousands of RL steps, DeepSeek-R1-Zero exhibits super '\n",
            "                     'performance on reasoning benchmarks. For instance, the '\n",
            "                     'pass@1 score on AIME 2024 increases from $15.6 \\\\%$ to '\n",
            "                     '$71.0 \\\\%$, and with majority voting, the score further '\n",
            "                     'improves to $86.7 \\\\%$, matching the performance of '\n",
            "                     'OpenAI-o1-0912.\\n'\n",
            "                     '\\n'\n",
            "                     'However, DeepSeek-R1-Zero encounters challenges such as '\n",
            "                     'poor readability, and language mixing. To address these '\n",
            "                     'issues and further enhance reasoning performance, we '\n",
            "                     'introduce DeepSeek-R1, which incorporates a small amount '\n",
            "                     'of cold-start data and a multi-stage training pipeline. '\n",
            "                     'Specifically, we begin by collecting thousands of '\n",
            "                     'cold-start data to fine-tune the DeepSeek-V3-Base model. '\n",
            "                     'Following this, we perform reasoning-oriented RL like '\n",
            "                     'DeepSeek-R1Zero. Upon nearing convergence in the RL '\n",
            "                     'process, we create new SFT data through rejection '\n",
            "                     'sampling on the RL checkpoint, combined with supervised '\n",
            "                     'data from DeepSeek-V3 in domains such as writing, '\n",
            "                     'factual QA, and self-cognition, and then retrain the '\n",
            "                     'DeepSeek-V3-Base model. After fine-tuning with the new '\n",
            "                     'data, the checkpoint undergoes an additional RL process, '\n",
            "                     'taking into account prompts from all scenarios. After '\n",
            "                     'these steps, we obtained a checkpoint referred to as '\n",
            "                     'DeepSeek-R1, which achieves performance on par with '\n",
            "                     'OpenAI-o1-1217.\\n'\n",
            "                     '\\n'\n",
            "                     'We further explore distillation from DeepSeek-R1 to '\n",
            "                     'smaller dense models. Using Qwen2.532B (Qwen, 2024b) as '\n",
            "                     'the base model, direct distillation from DeepSeek-R1 '\n",
            "                     'outperforms applying RL on it. This demonstrates that '\n",
            "                     'the reasoning patterns discovered by larger base models '\n",
            "                     'are crucial for improving reasoning capabilities. We '\n",
            "                     'open-source the distilled Qwen and Llama (Dubey et al., '\n",
            "                     '2024) series. Notably, our distilled 14B model '\n",
            "                     'outperforms state-of-the-art open-source QwQ-32B-Preview '\n",
            "                     '(Qwen, 2024a) by a large margin, and the distilled 32B '\n",
            "                     'and 70B models set a new record on the reasoning '\n",
            "                     'benchmarks among dense models.\\n',\n",
            "             'title': '1. Introduction'},\n",
            "            {'node_id': '0008',\n",
            "             'nodes': [{'node_id': '0009',\n",
            "                        'page_index': 5,\n",
            "                        'text': '### 2.1. Overview\\n'\n",
            "                                '\\n'\n",
            "                                'Previous work has heavily relied on large '\n",
            "                                'amounts of supervised data to enhance model '\n",
            "                                'performance. In this study, we demonstrate '\n",
            "                                'that reasoning capabilities can be '\n",
            "                                'significantly improved through large-scale '\n",
            "                                'reinforcement learning (RL), even without '\n",
            "                                'using supervised fine-tuning (SFT) as a cold '\n",
            "                                'start. Furthermore, performance can be '\n",
            "                                'further enhanced with the inclusion of a '\n",
            "                                'small amount of cold-start data. In the '\n",
            "                                'following sections, we present: (1) '\n",
            "                                'DeepSeek-R1-Zero, which applies RL directly '\n",
            "                                'to the base model without any SFT data, and '\n",
            "                                '(2) DeepSeek-R1, which applies RL starting '\n",
            "                                'from a checkpoint fine-tuned with thousands '\n",
            "                                'of long Chain-of-Thought (CoT) examples. 3) '\n",
            "                                'Distill the reasoning capability from '\n",
            "                                'DeepSeek-R1 to small dense models.\\n',\n",
            "                        'title': '2.1. Overview'},\n",
            "                       {'node_id': '0010',\n",
            "                        'nodes': [{'node_id': '0011',\n",
            "                                   'page_index': 5,\n",
            "                                   'text': '#### 2.2.1. Reinforcement Learning '\n",
            "                                           'Algorithm\\n'\n",
            "                                           '\\n'\n",
            "                                           'Group Relative Policy Optimization '\n",
            "                                           'In order to save the training '\n",
            "                                           'costs of RL, we adopt Group '\n",
            "                                           'Relative Policy Optimization '\n",
            "                                           '(GRPO) (Shao et al., 2024), which '\n",
            "                                           'foregoes the critic model that is '\n",
            "                                           'typically the same size as the '\n",
            "                                           'policy model, and estimates the '\n",
            "                                           'baseline from group scores '\n",
            "                                           'instead. Specifically, for each '\n",
            "                                           'question $q$, GRPO samples a group '\n",
            "                                           'of outputs $\\\\left\\\\{o_{1}, o_{2}, '\n",
            "                                           '\\\\cdots, o_{G}\\\\right\\\\}$ from the '\n",
            "                                           'old policy $\\\\pi_{\\\\theta_{\\\\text '\n",
            "                                           '{old }}}$ and then optimizes the '\n",
            "                                           'policy model $\\\\pi_{\\\\theta}$ by '\n",
            "                                           'maximizing the following '\n",
            "                                           'objective:\\n'\n",
            "                                           '\\n'\n",
            "                                           '$$\\n'\n",
            "                                           '\\\\begin{aligned}\\n'\n",
            "                                           '& \\\\mathcal{J}_{\\\\text {GRPO '\n",
            "                                           '}}(\\\\theta)=\\\\mathbb{E}\\\\left[q '\n",
            "                                           '\\\\sim '\n",
            "                                           'P(Q),\\\\left\\\\{o_{i}\\\\right\\\\}_{i=1}^{G} '\n",
            "                                           '\\\\sim \\\\pi_{\\\\theta_{\\\\text {old '\n",
            "                                           '}}}(O \\\\mid q)\\\\right] \\\\\\\\\\n'\n",
            "                                           '& \\\\frac{1}{G} '\n",
            "                                           '\\\\sum_{i=1}^{G}\\\\left(\\\\min '\n",
            "                                           '\\\\left(\\\\frac{\\\\pi_{\\\\theta}\\\\left(o_{i} '\n",
            "                                           '\\\\mid '\n",
            "                                           'q\\\\right)}{\\\\pi_{\\\\theta_{\\\\text '\n",
            "                                           '{old }}}\\\\left(o_{i} \\\\mid '\n",
            "                                           'q\\\\right)} A_{i}, '\n",
            "                                           '\\\\operatorname{clip}\\\\left(\\\\frac{\\\\pi_{\\\\theta}\\\\left(o_{i} '\n",
            "                                           '\\\\mid '\n",
            "                                           'q\\\\right)}{\\\\pi_{\\\\theta_{\\\\text '\n",
            "                                           '{old }}}\\\\left(o_{i} \\\\mid '\n",
            "                                           'q\\\\right)}, 1-\\\\varepsilon, '\n",
            "                                           '1+\\\\varepsilon\\\\right) '\n",
            "                                           'A_{i}\\\\right)-\\\\beta '\n",
            "                                           '\\\\mathbb{D}_{K '\n",
            "                                           'L}\\\\left(\\\\pi_{\\\\theta} \\\\| '\n",
            "                                           '\\\\pi_{r e f}\\\\right)\\\\right), '\n",
            "                                           '\\\\\\\\\\n'\n",
            "                                           '& \\\\mathbb{D}_{K '\n",
            "                                           'L}\\\\left(\\\\pi_{\\\\theta} \\\\| '\n",
            "                                           '\\\\pi_{r e '\n",
            "                                           'f}\\\\right)=\\\\frac{\\\\pi_{r e '\n",
            "                                           'f}\\\\left(o_{i} \\\\mid '\n",
            "                                           'q\\\\right)}{\\\\pi_{\\\\theta}\\\\left(o_{i} '\n",
            "                                           '\\\\mid q\\\\right)}-\\\\log '\n",
            "                                           '\\\\frac{\\\\pi_{r e f}\\\\left(o_{i} '\n",
            "                                           '\\\\mid '\n",
            "                                           'q\\\\right)}{\\\\pi_{\\\\theta}\\\\left(o_{i} '\n",
            "                                           '\\\\mid q\\\\right)}-1,\\n'\n",
            "                                           '\\\\end{aligned}\\n'\n",
            "                                           '$$\\n'\n",
            "                                           '\\n'\n",
            "                                           'where $\\\\varepsilon$ and $\\\\beta$ '\n",
            "                                           'are hyper-parameters, and $A_{i}$ '\n",
            "                                           'is the advantage, computed using a '\n",
            "                                           'group of rewards $\\\\left\\\\{r_{1}, '\n",
            "                                           'r_{2}, \\\\ldots, r_{G}\\\\right\\\\}$ '\n",
            "                                           'corresponding to the outputs '\n",
            "                                           'within each group:\\n'\n",
            "                                           '\\n'\n",
            "                                           '$$\\n'\n",
            "                                           'A_{i}=\\\\frac{r_{i}-\\\\operatorname{mean}\\\\left(\\\\left\\\\{r_{1}, '\n",
            "                                           'r_{2}, \\\\cdots, '\n",
            "                                           'r_{G}\\\\right\\\\}\\\\right)}{\\\\operatorname{std}\\\\left(\\\\left\\\\{r_{1}, '\n",
            "                                           'r_{2}, \\\\cdots, '\n",
            "                                           'r_{G}\\\\right\\\\}\\\\right)}\\n'\n",
            "                                           '$$\\n'\n",
            "                                           '\\n'\n",
            "                                           'A conversation between User and '\n",
            "                                           'Assistant. The user asks a '\n",
            "                                           'question, and the Assistant solves '\n",
            "                                           'it. The assistant first thinks '\n",
            "                                           'about the reasoning process in the '\n",
            "                                           'mind and then provides the user '\n",
            "                                           'with the answer. The reasoning '\n",
            "                                           'process and answer are enclosed '\n",
            "                                           'within <think> </think> and '\n",
            "                                           '<answer> </answer> tags, '\n",
            "                                           'respectively, i.e., <think> '\n",
            "                                           'reasoning process here </think> '\n",
            "                                           '<answer> answer here </answer>. '\n",
            "                                           'User: prompt. Assistant:\\n'\n",
            "                                           '\\n'\n",
            "                                           'Table 1 | Template for '\n",
            "                                           'DeepSeek-R1-Zero. prompt will be '\n",
            "                                           'replaced with the specific '\n",
            "                                           'reasoning question during '\n",
            "                                           'training.\\n',\n",
            "                                   'title': '2.2.1. Reinforcement Learning '\n",
            "                                            'Algorithm'},\n",
            "                                  {'node_id': '0012',\n",
            "                                   'page_index': 6,\n",
            "                                   'text': '#### 2.2.2. Reward Modeling\\n'\n",
            "                                           '\\n'\n",
            "                                           'The reward is the source of the '\n",
            "                                           'training signal, which decides the '\n",
            "                                           'optimization direction of RL. To '\n",
            "                                           'train DeepSeek-R1-Zero, we adopt a '\n",
            "                                           'rule-based reward system that '\n",
            "                                           'mainly consists of two types of '\n",
            "                                           'rewards:\\n'\n",
            "                                           '\\n'\n",
            "                                           '- Accuracy rewards: The accuracy '\n",
            "                                           'reward model evaluates whether the '\n",
            "                                           'response is correct. For example, '\n",
            "                                           'in the case of math problems with '\n",
            "                                           'deterministic results, the model '\n",
            "                                           'is required to provide the final '\n",
            "                                           'answer in a specified format '\n",
            "                                           '(e.g., within a box), enabling '\n",
            "                                           'reliable rule-based verification '\n",
            "                                           'of correctness. Similarly, for '\n",
            "                                           'LeetCode problems, a compiler can '\n",
            "                                           'be used to generate feedback based '\n",
            "                                           'on predefined test cases.\\n'\n",
            "                                           '- Format rewards: In addition to '\n",
            "                                           'the accuracy reward model, we '\n",
            "                                           'employ a format reward model that '\n",
            "                                           'enforces the model to put its '\n",
            "                                           \"thinking process between ' $<$ \"\n",
            "                                           \"think $>$ ' and ' $</$ think $>$ ' \"\n",
            "                                           'tags.\\n'\n",
            "                                           '\\n'\n",
            "                                           'We do not apply the outcome or '\n",
            "                                           'process neural reward model in '\n",
            "                                           'developing DeepSeek-R1-Zero, '\n",
            "                                           'because we find that the neural '\n",
            "                                           'reward model may suffer from '\n",
            "                                           'reward hacking in the large-scale '\n",
            "                                           'reinforcement learning process, '\n",
            "                                           'and retraining the reward model '\n",
            "                                           'needs additional training '\n",
            "                                           'resources and it complicates the '\n",
            "                                           'whole training pipeline.\\n',\n",
            "                                   'title': '2.2.2. Reward Modeling'},\n",
            "                                  {'node_id': '0013',\n",
            "                                   'page_index': 6,\n",
            "                                   'text': '#### 2.2.3. Training Template\\n'\n",
            "                                           '\\n'\n",
            "                                           'To train DeepSeek-R1-Zero, we '\n",
            "                                           'begin by designing a '\n",
            "                                           'straightforward template that '\n",
            "                                           'guides the base model to adhere to '\n",
            "                                           'our specified instructions. As '\n",
            "                                           'depicted in Table 1, this template '\n",
            "                                           'requires DeepSeek-R1-Zero to first '\n",
            "                                           'produce a reasoning process, '\n",
            "                                           'followed by the final answer. We '\n",
            "                                           'intentionally limit our '\n",
            "                                           'constraints to this structural '\n",
            "                                           'format, avoiding any '\n",
            "                                           'content-specific biases-such as '\n",
            "                                           'mandating reflective reasoning or '\n",
            "                                           'promoting particular '\n",
            "                                           'problem-solving strate-gies-to '\n",
            "                                           'ensure that we can accurately '\n",
            "                                           \"observe the model's natural \"\n",
            "                                           'progression during the RL '\n",
            "                                           'process.\\n'\n",
            "                                           '\\n'\n",
            "                                           '2.2.4. Performance, Self-evolution '\n",
            "                                           'Process and Aha Moment of '\n",
            "                                           'DeepSeek-R1-Zero\\n'\n",
            "                                           '\\n'\n",
            "                                           'Performance of DeepSeek-R1-Zero '\n",
            "                                           'Figure 2 depicts the performance '\n",
            "                                           'trajectory of DeepSeek-R1-Zero on '\n",
            "                                           'the AIME 2024 benchmark throughout '\n",
            "                                           'the RL training process. As '\n",
            "                                           'illustrated, DeepSeek-R1-Zero '\n",
            "                                           'demonstrates a steady and '\n",
            "                                           'consistent enhancement in '\n",
            "                                           'performance as the RL training '\n",
            "                                           'advances. Notably, the average '\n",
            "                                           'pass@1 score on AIME 2024 shows a '\n",
            "                                           'significant increase, jumping from '\n",
            "                                           'an initial $15.6 \\\\%$ to an '\n",
            "                                           'impressive $71.0 \\\\%$, reaching '\n",
            "                                           'performance levels comparable to '\n",
            "                                           'OpenAI-o1-0912. This significant '\n",
            "                                           'improvement highlights the '\n",
            "                                           'efficacy of our RL algorithm in '\n",
            "                                           \"optimizing the model's performance \"\n",
            "                                           'over time.\\n'\n",
            "                                           '\\n'\n",
            "                                           'Table 2 provides a comparative '\n",
            "                                           'analysis between DeepSeek-R1-Zero '\n",
            "                                           \"and OpenAI's o1-0912 models across \"\n",
            "                                           'a variety of reasoning-related '\n",
            "                                           'benchmarks. The findings reveal '\n",
            "                                           'that RL empowers\\n'\n",
            "                                           '\\n'\n",
            "                                           '| Model | AIME 2024 | | MATH-500 | '\n",
            "                                           'GPQA <br> Diamond | LiveCode <br> '\n",
            "                                           'Bench | CodeForces |\\n'\n",
            "                                           '| :-- | :--: | :--: | :--: | :--: '\n",
            "                                           '| :--: | :--: |\\n'\n",
            "                                           '|  | pass@1 | cons@64 | pass@1 | '\n",
            "                                           'pass@1 | pass@1 | rating |\\n'\n",
            "                                           '| OpenAI-o1-mini | 63.6 | 80.0 | '\n",
            "                                           '90.0 | 60.0 | 53.8 | 1820 |\\n'\n",
            "                                           '| OpenAI-o1-0912 | 74.4 | 83.3 | '\n",
            "                                           '94.8 | 77.3 | 63.4 | 1843 |\\n'\n",
            "                                           '| DeepSeek-R1-Zero | 71.0 | 86.7 | '\n",
            "                                           '95.9 | 73.3 | 50.0 | 1444 |\\n'\n",
            "                                           '\\n'\n",
            "                                           'Table 2 | Comparison of '\n",
            "                                           'DeepSeek-R1-Zero and OpenAI o1 '\n",
            "                                           'models on reasoning-related '\n",
            "                                           'benchmarks.\\n'\n",
            "                                           '![img-1.jpeg](img-1.jpeg)\\n'\n",
            "                                           '\\n'\n",
            "                                           'Figure 2 | AIME accuracy of '\n",
            "                                           'DeepSeek-R1-Zero during training. '\n",
            "                                           'For each question, we sample 16 '\n",
            "                                           'responses and calculate the '\n",
            "                                           'overall average accuracy to ensure '\n",
            "                                           'a stable evaluation.\\n'\n",
            "                                           '\\n'\n",
            "                                           'DeepSeek-R1-Zero to attain robust '\n",
            "                                           'reasoning capabilities without the '\n",
            "                                           'need for any supervised '\n",
            "                                           'fine-tuning data. This is a '\n",
            "                                           'noteworthy achievement, as it '\n",
            "                                           \"underscores the model's ability to \"\n",
            "                                           'learn and generalize effectively '\n",
            "                                           'through RL alone. Additionally, '\n",
            "                                           'the performance of DeepSeekR1-Zero '\n",
            "                                           'can be further augmented through '\n",
            "                                           'the application of majority '\n",
            "                                           'voting. For example, when majority '\n",
            "                                           'voting is employed on the AIME '\n",
            "                                           \"benchmark, DeepSeek-R1-Zero's \"\n",
            "                                           'performance escalates from $71.0 '\n",
            "                                           '\\\\%$ to $86.7 \\\\%$, thereby '\n",
            "                                           'exceeding the performance of '\n",
            "                                           'OpenAI-o1-0912. The ability of '\n",
            "                                           'DeepSeek-R1-Zero to achieve such '\n",
            "                                           'competitive performance, both with '\n",
            "                                           'and without majority voting, '\n",
            "                                           'highlights its strong foundational '\n",
            "                                           'capabilities and its potential for '\n",
            "                                           'further advancements in reasoning '\n",
            "                                           'tasks.\\n'\n",
            "                                           '\\n'\n",
            "                                           'Self-evolution Process of '\n",
            "                                           'DeepSeek-R1-Zero The '\n",
            "                                           'self-evolution process of '\n",
            "                                           'DeepSeek-R1-Zero is a fascinating '\n",
            "                                           'demonstration of how RL can drive '\n",
            "                                           'a model to improve its reasoning '\n",
            "                                           'capabilities autonomously. By '\n",
            "                                           'initiating RL directly from the '\n",
            "                                           'base model, we can closely monitor '\n",
            "                                           \"the model's progression without \"\n",
            "                                           'the influence of the supervised '\n",
            "                                           'fine-tuning stage. This approach '\n",
            "                                           'provides a clear view of how the '\n",
            "                                           'model evolves over time, '\n",
            "                                           'particularly in terms of its '\n",
            "                                           'ability to handle complex '\n",
            "                                           'reasoning tasks.\\n'\n",
            "                                           '\\n'\n",
            "                                           'As depicted in Figure 3, the '\n",
            "                                           'thinking time of DeepSeek-R1-Zero '\n",
            "                                           'shows consistent improve-\\n'\n",
            "                                           '\\n'\n",
            "                                           '![img-2.jpeg](img-2.jpeg)\\n'\n",
            "                                           '\\n'\n",
            "                                           'Figure 3 | The average response '\n",
            "                                           'length of DeepSeek-R1-Zero on the '\n",
            "                                           'training set during the RL '\n",
            "                                           'process. DeepSeek-R1-Zero '\n",
            "                                           'naturally learns to solve '\n",
            "                                           'reasoning tasks with more thinking '\n",
            "                                           'time.\\n'\n",
            "                                           'ment throughout the training '\n",
            "                                           'process. This improvement is not '\n",
            "                                           'the result of external adjustments '\n",
            "                                           'but rather an intrinsic '\n",
            "                                           'development within the model. '\n",
            "                                           'DeepSeek-R1-Zero naturally '\n",
            "                                           'acquires the ability to solve '\n",
            "                                           'increasingly complex reasoning '\n",
            "                                           'tasks by leveraging extended '\n",
            "                                           'test-time computation. This '\n",
            "                                           'computation ranges from generating '\n",
            "                                           'hundreds to thousands of reasoning '\n",
            "                                           'tokens, allowing the model to '\n",
            "                                           'explore and refine its thought '\n",
            "                                           'processes in greater depth.\\n'\n",
            "                                           '\\n'\n",
            "                                           'One of the most remarkable aspects '\n",
            "                                           'of this self-evolution is the '\n",
            "                                           'emergence of sophisticated '\n",
            "                                           'behaviors as the test-time '\n",
            "                                           'computation increases. Behaviors '\n",
            "                                           'such as reflection-where the model '\n",
            "                                           'revisits and reevaluates its '\n",
            "                                           'previous steps-and the exploration '\n",
            "                                           'of alternative approaches to '\n",
            "                                           'problem-solving arise '\n",
            "                                           'spontaneously. These behaviors are '\n",
            "                                           'not explicitly programmed but '\n",
            "                                           'instead emerge as a result of the '\n",
            "                                           \"model's interaction with the \"\n",
            "                                           'reinforcement learning '\n",
            "                                           'environment. This spontaneous '\n",
            "                                           'development significantly enhances '\n",
            "                                           \"DeepSeek-R1-Zero's reasoning \"\n",
            "                                           'capabilities, enabling it to '\n",
            "                                           'tackle more challenging tasks with '\n",
            "                                           'greater efficiency and accuracy.\\n'\n",
            "                                           '\\n'\n",
            "                                           'Aha Moment of DeepSeek-R1-Zero A '\n",
            "                                           'particularly intriguing phenomenon '\n",
            "                                           'observed during the training of '\n",
            "                                           'DeepSeek-R1-Zero is the occurrence '\n",
            "                                           'of an \"aha moment\". This moment, '\n",
            "                                           'as illustrated in Table 3, occurs '\n",
            "                                           'in an intermediate version of the '\n",
            "                                           'model. During this phase, '\n",
            "                                           'DeepSeek-R1-Zero learns to '\n",
            "                                           'allocate more thinking time to a '\n",
            "                                           'problem by reevaluating its '\n",
            "                                           'initial approach. This behavior is '\n",
            "                                           'not only a testament to the '\n",
            "                                           \"model's growing reasoning \"\n",
            "                                           'abilities but also a captivating '\n",
            "                                           'example of how reinforcement '\n",
            "                                           'learning can lead to unexpected '\n",
            "                                           'and sophisticated outcomes.\\n'\n",
            "                                           '\\n'\n",
            "                                           'This moment is not only an \"aha '\n",
            "                                           'moment\" for the model but also for '\n",
            "                                           'the researchers observing its '\n",
            "                                           'behavior. It underscores the power '\n",
            "                                           'and beauty of reinforcement '\n",
            "                                           'learning: rather than explicitly '\n",
            "                                           'teaching the model on how to solve '\n",
            "                                           'a problem, we simply provide it '\n",
            "                                           'with the right incentives, and it '\n",
            "                                           'autonomously develops advanced '\n",
            "                                           'problem-solving strategies. The '\n",
            "                                           '\"aha moment\" serves as a powerful '\n",
            "                                           'reminder of the potential of RL to '\n",
            "                                           'unlock new levels of intelligence '\n",
            "                                           'in artificial systems, paving the '\n",
            "                                           'way for more autonomous and '\n",
            "                                           'adaptive models in the future.\\n'\n",
            "                                           '\\n'\n",
            "                                           'Question: If $a>1$, then the sum '\n",
            "                                           'of the real solutions of '\n",
            "                                           '$\\\\sqrt{a-\\\\sqrt{a+x}}=x$ is equal '\n",
            "                                           'to\\n'\n",
            "                                           'Response: <think>\\n'\n",
            "                                           'To solve the equation '\n",
            "                                           \"$\\\\sqrt{a-\\\\sqrt{a+x}}=x$, let's \"\n",
            "                                           'start by squaring both $\\\\cdots$\\n'\n",
            "                                           '$(\\\\sqrt{a-\\\\sqrt{a+x}})^{2}=x^{2} '\n",
            "                                           '\\\\Longrightarrow '\n",
            "                                           'a-\\\\sqrt{a+x}=x^{2}$.\\n'\n",
            "                                           'Rearrange to isolate the inner '\n",
            "                                           'square root term:\\n'\n",
            "                                           '$\\\\left(a-x^{2}\\\\right)^{2}=a+x '\n",
            "                                           '\\\\Longrightarrow a^{2}-2 a '\n",
            "                                           'x^{2}+\\\\left(x^{2}\\\\right)^{2}=a+x '\n",
            "                                           '\\\\Longrightarrow x^{4}-2 a '\n",
            "                                           'x^{2}-x+\\\\left(a^{2}-a\\\\right)=0$\\n'\n",
            "                                           '...\\n'\n",
            "                                           \"Wait, wait. Wait. That's an aha \"\n",
            "                                           'moment I can flag here.\\n'\n",
            "                                           \"Let's reevaluate this step-by-step \"\n",
            "                                           'to identify if the correct sum can '\n",
            "                                           'be ...\\n'\n",
            "                                           'We started with the equation:\\n'\n",
            "                                           '$\\\\sqrt{a-\\\\sqrt{a+x}}=x$\\n'\n",
            "                                           \"First, let's square both sides:\\n\"\n",
            "                                           '$a-\\\\sqrt{a+x}=x^{2} '\n",
            "                                           '\\\\Longrightarrow '\n",
            "                                           '\\\\sqrt{a+x}=a-x^{2}$\\n'\n",
            "                                           'Next, I could square both sides '\n",
            "                                           'again, treating the equation: ...\\n'\n",
            "                                           '...\\n'\n",
            "                                           'Table 3 | An interesting \"aha '\n",
            "                                           'moment\" of an intermediate version '\n",
            "                                           'of DeepSeek-R1-Zero. The model '\n",
            "                                           'learns to rethink using an '\n",
            "                                           'anthropomorphic tone. This is also '\n",
            "                                           'an aha moment for us, allowing us '\n",
            "                                           'to witness the power and beauty of '\n",
            "                                           'reinforcement learning.\\n'\n",
            "                                           '\\n'\n",
            "                                           'Drawback of DeepSeek-R1-Zero '\n",
            "                                           'Although DeepSeek-R1-Zero exhibits '\n",
            "                                           'strong reasoning capabilities and '\n",
            "                                           'autonomously develops unexpected '\n",
            "                                           'and powerful reasoning behaviors, '\n",
            "                                           'it faces several issues. For '\n",
            "                                           'instance, DeepSeek-R1-Zero '\n",
            "                                           'struggles with challenges like '\n",
            "                                           'poor readability, and language '\n",
            "                                           'mixing. To make reasoning '\n",
            "                                           'processes more readable and share '\n",
            "                                           'them with the open community, we '\n",
            "                                           'explore DeepSeek-R1, a method that '\n",
            "                                           'utilizes RL with human-friendly '\n",
            "                                           'cold-start data.\\n',\n",
            "                                   'title': '2.2.3. Training Template'}],\n",
            "                        'page_index': 5,\n",
            "                        'text': '### 2.2. DeepSeek-R1-Zero: Reinforcement '\n",
            "                                'Learning on the Base Model\\n'\n",
            "                                '\\n'\n",
            "                                'Reinforcement learning has demonstrated '\n",
            "                                'significant effectiveness in reasoning tasks, '\n",
            "                                'as evidenced by our previous works (Shao et '\n",
            "                                'al., 2024; Wang et al., 2023). However, these '\n",
            "                                'works heavily depended on supervised data, '\n",
            "                                'which are time-intensive to gather. In this '\n",
            "                                'section, we explore the potential of LLMs to '\n",
            "                                'develop reasoning capabilities without any '\n",
            "                                'supervised data, focusing on their '\n",
            "                                'self-evolution through a pure reinforcement '\n",
            "                                'learning process. We start with a brief '\n",
            "                                'overview of our RL algorithm, followed by the '\n",
            "                                'presentation of some exciting results, and '\n",
            "                                'hope this provides the community with '\n",
            "                                'valuable insights.\\n',\n",
            "                        'title': '2.2. DeepSeek-R1-Zero: Reinforcement '\n",
            "                                 'Learning on the Base Model'},\n",
            "                       {'node_id': '0014',\n",
            "                        'nodes': [{'node_id': '0015',\n",
            "                                   'page_index': 9,\n",
            "                                   'text': '#### 2.3.1. Cold Start\\n'\n",
            "                                           '\\n'\n",
            "                                           'Unlike DeepSeek-R1-Zero, to '\n",
            "                                           'prevent the early unstable cold '\n",
            "                                           'start phase of RL training from '\n",
            "                                           'the base model, for DeepSeek-R1 we '\n",
            "                                           'construct and collect a small '\n",
            "                                           'amount of long CoT data to '\n",
            "                                           'fine-tune the model as the initial '\n",
            "                                           'RL actor. To collect such data, we '\n",
            "                                           'have explored several approaches: '\n",
            "                                           'using few-shot prompting with a '\n",
            "                                           'long CoT as an example, directly '\n",
            "                                           'prompting models to generate '\n",
            "                                           'detailed answers with reflection '\n",
            "                                           'and verification, gathering '\n",
            "                                           'DeepSeek-R1Zero outputs in a '\n",
            "                                           'readable format, and refining the '\n",
            "                                           'results through post-processing by '\n",
            "                                           'human annotators.\\n'\n",
            "                                           '\\n'\n",
            "                                           'In this work, we collect thousands '\n",
            "                                           'of cold-start data to fine-tune '\n",
            "                                           'the DeepSeek-V3-Base as the '\n",
            "                                           'starting point for RL. Compared to '\n",
            "                                           'DeepSeek-R1-Zero, the advantages '\n",
            "                                           'of cold start data\\n'\n",
            "                                           '\\n'\n",
            "                                           'include:\\n'\n",
            "                                           '\\n'\n",
            "                                           '- Readability: A key limitation of '\n",
            "                                           'DeepSeek-R1-Zero is that its '\n",
            "                                           'content is often not suitable for '\n",
            "                                           'reading. Responses may mix '\n",
            "                                           'multiple languages or lack '\n",
            "                                           'markdown formatting to highlight '\n",
            "                                           'answers for users. In contrast, '\n",
            "                                           'when creating cold-start data for '\n",
            "                                           'DeepSeek-R1, we design a readable '\n",
            "                                           'pattern that includes a summary at '\n",
            "                                           'the end of each response and '\n",
            "                                           'filters out responses that are not '\n",
            "                                           'reader-friendly. Here, we define '\n",
            "                                           'the output format as I '\n",
            "                                           'special_token I '\n",
            "                                           '<reasoning_process> I '\n",
            "                                           'special_token I <summary>, where '\n",
            "                                           'the reasoning process is the CoT '\n",
            "                                           'for the query, and the summary is '\n",
            "                                           'used to summarize the reasoning '\n",
            "                                           'results.\\n'\n",
            "                                           '- Potential: By carefully '\n",
            "                                           'designing the pattern for '\n",
            "                                           'cold-start data with human priors, '\n",
            "                                           'we observe better performance '\n",
            "                                           'against DeepSeek-R1-Zero. We '\n",
            "                                           'believe the iterative training is '\n",
            "                                           'a better way for reasoning '\n",
            "                                           'models.\\n',\n",
            "                                   'title': '2.3.1. Cold Start'},\n",
            "                                  {'node_id': '0016',\n",
            "                                   'page_index': 10,\n",
            "                                   'text': '#### 2.3.2. Reasoning-oriented '\n",
            "                                           'Reinforcement Learning\\n'\n",
            "                                           '\\n'\n",
            "                                           'After fine-tuning DeepSeek-V3-Base '\n",
            "                                           'on the cold start data, we apply '\n",
            "                                           'the same large-scale reinforcement '\n",
            "                                           'learning training process as '\n",
            "                                           'employed in DeepSeek-R1-Zero. This '\n",
            "                                           'phase focuses on enhancing the '\n",
            "                                           \"model's reasoning capabilities, \"\n",
            "                                           'particularly in '\n",
            "                                           'reasoning-intensive tasks such as '\n",
            "                                           'coding, mathematics, science, and '\n",
            "                                           'logic reasoning, which involve '\n",
            "                                           'well-defined problems with clear '\n",
            "                                           'solutions. During the training '\n",
            "                                           'process, we observe that CoT often '\n",
            "                                           'exhibits language mixing, '\n",
            "                                           'particularly when RL prompts '\n",
            "                                           'involve multiple languages. To '\n",
            "                                           'mitigate the issue of language '\n",
            "                                           'mixing, we introduce a language '\n",
            "                                           'consistency reward during RL '\n",
            "                                           'training, which is calculated as '\n",
            "                                           'the proportion of target language '\n",
            "                                           'words in the CoT. Although '\n",
            "                                           'ablation experiments show that '\n",
            "                                           'such alignment results in a slight '\n",
            "                                           \"degradation in the model's \"\n",
            "                                           'performance, this reward aligns '\n",
            "                                           'with human preferences, making it '\n",
            "                                           'more readable. Finally, we combine '\n",
            "                                           'the accuracy of reasoning tasks '\n",
            "                                           'and the reward for language '\n",
            "                                           'consistency by directly summing '\n",
            "                                           'them to form the final reward. We '\n",
            "                                           'then apply RL training on the '\n",
            "                                           'fine-tuned model until it achieves '\n",
            "                                           'convergence on reasoning tasks.\\n',\n",
            "                                   'title': '2.3.2. Reasoning-oriented '\n",
            "                                            'Reinforcement Learning'},\n",
            "                                  {'node_id': '0017',\n",
            "                                   'page_index': 10,\n",
            "                                   'text': '#### 2.3.3. Rejection Sampling and '\n",
            "                                           'Supervised Fine-Tuning\\n'\n",
            "                                           '\\n'\n",
            "                                           'When reasoning-oriented RL '\n",
            "                                           'converges, we utilize the '\n",
            "                                           'resulting checkpoint to collect '\n",
            "                                           'SFT (Supervised Fine-Tuning) data '\n",
            "                                           'for the subsequent round. Unlike '\n",
            "                                           'the initial cold-start data, which '\n",
            "                                           'primarily focuses on reasoning, '\n",
            "                                           'this stage incorporates data from '\n",
            "                                           'other domains to enhance the '\n",
            "                                           \"model's capabilities in writing, \"\n",
            "                                           'role-playing, and other '\n",
            "                                           'general-purpose tasks. '\n",
            "                                           'Specifically, we generate the data '\n",
            "                                           'and fine-tune the model as '\n",
            "                                           'described below.\\n'\n",
            "                                           '\\n'\n",
            "                                           'Reasoning data We curate reasoning '\n",
            "                                           'prompts and generate reasoning '\n",
            "                                           'trajectories by performing '\n",
            "                                           'rejection sampling from the '\n",
            "                                           'checkpoint from the above RL '\n",
            "                                           'training. In the previous stage, '\n",
            "                                           'we only included data that could '\n",
            "                                           'be evaluated using rule-based '\n",
            "                                           'rewards. However, in this stage, '\n",
            "                                           'we expand the dataset by '\n",
            "                                           'incorporating additional data, '\n",
            "                                           'some of which use a generative '\n",
            "                                           'reward model by feeding the '\n",
            "                                           'ground-truth and model predictions '\n",
            "                                           'into DeepSeek-V3 for judgment. '\n",
            "                                           'Additionally, because the model '\n",
            "                                           'output is sometimes chaotic and '\n",
            "                                           'difficult to read, we have '\n",
            "                                           'filtered out chain-of-thought with '\n",
            "                                           'mixed languages, long parapraphs, '\n",
            "                                           'and code blocks. For each prompt, '\n",
            "                                           'we sample multiple responses and '\n",
            "                                           'retain only the correct ones. In '\n",
            "                                           'total, we collect about 600 k '\n",
            "                                           'reasoning related training '\n",
            "                                           'samples.\\n'\n",
            "                                           '\\n'\n",
            "                                           'Non-Reasoning data For '\n",
            "                                           'non-reasoning data, such as '\n",
            "                                           'writing, factual QA, '\n",
            "                                           'self-cognition, and translation, '\n",
            "                                           'we adopt the DeepSeek-V3 pipeline '\n",
            "                                           'and reuse portions of the SFT '\n",
            "                                           'dataset of DeepSeek-V3. For '\n",
            "                                           'certain non-reasoning tasks, we '\n",
            "                                           'call DeepSeek-V3 to generate a '\n",
            "                                           'potential chain-of-thought before '\n",
            "                                           'answering the question by '\n",
            "                                           'prompting. However, for simpler '\n",
            "                                           'queries, such as \"hello\" we do not '\n",
            "                                           'provide a CoT in response. In the '\n",
            "                                           'end, we collected a total of '\n",
            "                                           'approximately 200k training '\n",
            "                                           'samples that are unrelated to '\n",
            "                                           'reasoning.\\n'\n",
            "                                           '\\n'\n",
            "                                           'We fine-tune DeepSeek-V3-Base for '\n",
            "                                           'two epochs using the above curated '\n",
            "                                           'dataset of about 800k samples.\\n',\n",
            "                                   'title': '2.3.3. Rejection Sampling and '\n",
            "                                            'Supervised Fine-Tuning'},\n",
            "                                  {'node_id': '0018',\n",
            "                                   'page_index': 11,\n",
            "                                   'text': '#### 2.3.4. Reinforcement Learning '\n",
            "                                           'for all Scenarios\\n'\n",
            "                                           '\\n'\n",
            "                                           'To further align the model with '\n",
            "                                           'human preferences, we implement a '\n",
            "                                           'secondary reinforcement learning '\n",
            "                                           'stage aimed at improving the '\n",
            "                                           \"model's helpfulness and \"\n",
            "                                           'harmlessness while simultaneously '\n",
            "                                           'refining its reasoning '\n",
            "                                           'capabilities. Specifically, we '\n",
            "                                           'train the model using a '\n",
            "                                           'combination of reward signals and '\n",
            "                                           'diverse prompt distributions. For '\n",
            "                                           'reasoning data, we adhere to the '\n",
            "                                           'methodology outlined in '\n",
            "                                           'DeepSeek-R1-Zero, which utilizes '\n",
            "                                           'rule-based rewards to guide the '\n",
            "                                           'learning process in math, code, '\n",
            "                                           'and logical reasoning domains. For '\n",
            "                                           'general data, we resort to reward '\n",
            "                                           'models to capture human '\n",
            "                                           'preferences in complex and nuanced '\n",
            "                                           'scenarios. We build upon the '\n",
            "                                           'DeepSeek-V3 pipeline and adopt a '\n",
            "                                           'similar distribution of preference '\n",
            "                                           'pairs and training prompts. For '\n",
            "                                           'helpfulness, we focus exclusively '\n",
            "                                           'on the final summary, ensuring '\n",
            "                                           'that the assessment emphasizes the '\n",
            "                                           'utility and relevance of the '\n",
            "                                           'response to the user while '\n",
            "                                           'minimizing interference with the '\n",
            "                                           'underlying reasoning process. For '\n",
            "                                           'harmlessness, we evaluate the '\n",
            "                                           'entire response of the model, '\n",
            "                                           'including both the reasoning '\n",
            "                                           'process and the summary, to '\n",
            "                                           'identify and mitigate any '\n",
            "                                           'potential risks, biases, or '\n",
            "                                           'harmful content that may arise '\n",
            "                                           'during the generation process. '\n",
            "                                           'Ultimately, the integration of '\n",
            "                                           'reward signals and diverse data '\n",
            "                                           'distributions enables us to train '\n",
            "                                           'a model that excels in reasoning '\n",
            "                                           'while prioritizing helpfulness and '\n",
            "                                           'harmlessness.\\n',\n",
            "                                   'title': '2.3.4. Reinforcement Learning for '\n",
            "                                            'all Scenarios'}],\n",
            "                        'page_index': 9,\n",
            "                        'text': '### 2.3. DeepSeek-R1: Reinforcement Learning '\n",
            "                                'with Cold Start\\n'\n",
            "                                '\\n'\n",
            "                                'Inspired by the promising results of '\n",
            "                                'DeepSeek-R1-Zero, two natural questions '\n",
            "                                'arise: 1) Can reasoning performance be '\n",
            "                                'further improved or convergence accelerated '\n",
            "                                'by incorporating a small amount of '\n",
            "                                'high-quality data as a cold start? 2) How can '\n",
            "                                'we train a user-friendly model that not only '\n",
            "                                'produces clear and coherent Chains of Thought '\n",
            "                                '(CoT) but also demonstrates strong general '\n",
            "                                'capabilities? To address these questions, we '\n",
            "                                'design a pipeline to train DeepSeek-R1. The '\n",
            "                                'pipeline consists of four stages, outlined as '\n",
            "                                'follows.\\n',\n",
            "                        'title': '2.3. DeepSeek-R1: Reinforcement Learning '\n",
            "                                 'with Cold Start'},\n",
            "                       {'node_id': '0019',\n",
            "                        'page_index': 11,\n",
            "                        'text': '### 2.4. Distillation: Empower Small Models '\n",
            "                                'with Reasoning Capability\\n'\n",
            "                                '\\n'\n",
            "                                'To equip more efficient smaller models with '\n",
            "                                'reasoning capabilities like DeepSeek-R1, we '\n",
            "                                'directly fine-tuned open-source models like '\n",
            "                                'Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) '\n",
            "                                'using the 800k samples curated with '\n",
            "                                'DeepSeek-R1, as detailed in ¬ß2.3.3. Our '\n",
            "                                'findings indicate that this straightforward '\n",
            "                                'distillation method significantly enhances '\n",
            "                                'the reasoning abilities of smaller models. '\n",
            "                                'The base models we use here are '\n",
            "                                'Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, '\n",
            "                                'Qwen2.514B, Qwen2.5-32B, Llama-3.1-8B, and '\n",
            "                                'Llama-3.3-70B-Instruct. We select Llama-3.3 '\n",
            "                                'because its reasoning capability is slightly '\n",
            "                                'better than that of Llama-3.1.\\n'\n",
            "                                '\\n'\n",
            "                                'For distilled models, we apply only SFT and '\n",
            "                                'do not include an RL stage, even though '\n",
            "                                'incorporating RL could substantially boost '\n",
            "                                'model performance. Our primary goal here is '\n",
            "                                'to demonstrate the effectiveness of the '\n",
            "                                'distillation technique, leaving the '\n",
            "                                'exploration of the RL stage to the broader '\n",
            "                                'research community.\\n',\n",
            "                        'title': '2.4. Distillation: Empower Small Models with '\n",
            "                                 'Reasoning Capability'}],\n",
            "             'page_index': 5,\n",
            "             'text': '## 2. Approach\\n',\n",
            "             'title': '2. Approach'},\n",
            "            {'node_id': '0020',\n",
            "             'nodes': [{'node_id': '0021',\n",
            "                        'page_index': 13,\n",
            "                        'text': '### 3.1. DeepSeek-R1 Evaluation\\n'\n",
            "                                '\\n'\n",
            "                                '|  | Benchmark (Metric) | Claude-3.5- <br> '\n",
            "                                'Sonnet-1022 | GPT-4o <br> 0513 | DeepSeek V3 '\n",
            "                                '| OpenAI <br> o1-mini | OpenAI <br> o1-1217 | '\n",
            "                                'DeepSeek <br> R1 |\\n'\n",
            "                                '| :--: | :--: | :--: | :--: | :--: | :--: | '\n",
            "                                ':--: | :--: |\\n'\n",
            "                                '|  | Architecture | - | - | MoE | - | - | MoE '\n",
            "                                '|\\n'\n",
            "                                '|  | \\\\# Activated Params | - | - | 37B | - | '\n",
            "                                '- | 37B |\\n'\n",
            "                                '|  | \\\\# Total Params | - | - | 671B | - | - '\n",
            "                                '| 671B |\\n'\n",
            "                                '| English | MMLU (PassR1) | 88.3 | 87.2 | '\n",
            "                                '88.5 | 85.2 | 91.8 | 90.8 |\\n'\n",
            "                                '|  | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | '\n",
            "                                '86.7 | - | 92.9 |\\n'\n",
            "                                '|  | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | '\n",
            "                                '80.3 | - | 84.0 |\\n'\n",
            "                                '|  | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | '\n",
            "                                '83.9 | 90.2 | 92.2 |\\n'\n",
            "                                '|  | IF-Eval (Prompt Strict) | 86.5 | 84.3 | '\n",
            "                                '86.1 | 84.8 | - | 83.3 |\\n'\n",
            "                                '|  | GPQA Diamond (PassR1) | 65.0 | 49.9 | '\n",
            "                                '59.1 | 60.0 | 75.7 | 71.5 |\\n'\n",
            "                                '|  | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 '\n",
            "                                '| 7.0 | 47.0 | 30.1 |\\n'\n",
            "                                '|  | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | '\n",
            "                                '76.9 | - | 82.5 |\\n'\n",
            "                                '|  | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 '\n",
            "                                '| 70.0 | 57.8 | - | 87.6 |\\n'\n",
            "                                '|  | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | '\n",
            "                                '85.5 | 92.0 | - | 92.3 |\\n'\n",
            "                                '| Code | LiveCodeBench (PassR1-COT) | 38.9 | '\n",
            "                                '32.9 | 36.2 | 53.8 | 63.4 | 65.9 |\\n'\n",
            "                                '|  | Codeforces (Percentile) | 20.3 | 23.6 | '\n",
            "                                '58.7 | 93.4 | 96.6 | 96.3 |\\n'\n",
            "                                '|  | Codeforces (Rating) | 717 | 759 | 1134 | '\n",
            "                                '1820 | 2061 | 2029 |\\n'\n",
            "                                '|  | SWE Verified (Resolved) | 50.8 | 38.8 | '\n",
            "                                '42.0 | 41.6 | 48.9 | 49.2 |\\n'\n",
            "                                '|  | Aider-Polyglot (Acc.) | 45.3 | 16.0 | '\n",
            "                                '49.6 | 32.9 | 61.7 | 53.3 |\\n'\n",
            "                                '| Math | AIME 2024 (PassR1) | 16.0 | 9.3 | '\n",
            "                                '39.2 | 63.6 | 79.2 | 79.8 |\\n'\n",
            "                                '|  | MATH-500 (PassR1) | 78.3 | 74.6 | 90.2 | '\n",
            "                                '90.0 | 96.4 | 97.3 |\\n'\n",
            "                                '|  | CNMO 2024 (PassR1) | 13.1 | 10.8 | 43.2 '\n",
            "                                '| 67.6 | - | 78.8 |\\n'\n",
            "                                '| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 '\n",
            "                                '| 89.9 | - | 92.8 |\\n'\n",
            "                                '|  | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 '\n",
            "                                '| - | 91.8 |\\n'\n",
            "                                '|  | C-SimpleQA (Correct) | 55.4 | 58.7 | '\n",
            "                                '68.0 | 40.3 | - | 63.7 |\\n'\n",
            "                                '\\n'\n",
            "                                'Table 4 | Comparison between DeepSeek-R1 and '\n",
            "                                'other representative models.\\n'\n",
            "                                'For education-oriented knowledge benchmarks '\n",
            "                                'such as MMLU, MMLU-Pro, and GPQA Diamond, '\n",
            "                                'DeepSeek-R1 demonstrates superior performance '\n",
            "                                'compared to DeepSeek-V3. This improvement is '\n",
            "                                'primarily attributed to enhanced accuracy in '\n",
            "                                'STEM-related questions, where significant '\n",
            "                                'gains are achieved through large-scale '\n",
            "                                'reinforcement learning. Additionally, '\n",
            "                                'DeepSeek-R1 excels on FRAMES, a '\n",
            "                                'long-context-dependent QA task, showcasing '\n",
            "                                'its strong document analysis capabilities. '\n",
            "                                'This highlights the potential of reasoning '\n",
            "                                'models in AI-driven search and data analysis '\n",
            "                                'tasks. On the factual benchmark SimpleQA, '\n",
            "                                'DeepSeek-R1 outperforms DeepSeek-V3, '\n",
            "                                'demonstrating its capability in handling '\n",
            "                                'fact-based queries. A similar trend is '\n",
            "                                'observed where OpenAI-o1 surpasses GPT-4o on '\n",
            "                                'this benchmark. However, DeepSeek-R1 performs '\n",
            "                                'worse than DeepSeek-V3 on the Chinese '\n",
            "                                'SimpleQA benchmark, primarily due to its '\n",
            "                                'tendency to refuse answering certain queries '\n",
            "                                'after safety RL. Without safety RL, '\n",
            "                                'DeepSeek-R1 could achieve an accuracy of over '\n",
            "                                '$70 \\\\%$.\\n'\n",
            "                                '\\n'\n",
            "                                'DeepSeek-R1 also delivers impressive results '\n",
            "                                'on IF-Eval, a benchmark designed to assess a '\n",
            "                                \"model's ability to follow format \"\n",
            "                                'instructions. These improvements can be '\n",
            "                                'linked to the inclusion of '\n",
            "                                'instruction-following data during the final '\n",
            "                                'stages of supervised fine-tuning (SFT) and RL '\n",
            "                                'training. Furthermore, remarkable performance '\n",
            "                                'is observed on AlpacaEval2.0 and ArenaHard, '\n",
            "                                \"indicating DeepSeek-R1's strengths in writing \"\n",
            "                                'tasks and open-domain question answering. Its '\n",
            "                                'significant outperformance of DeepSeek-V3 '\n",
            "                                'underscores the generalization benefits of '\n",
            "                                'large-scale RL, which not only boosts '\n",
            "                                'reasoning capabilities but also improves '\n",
            "                                'performance across diverse domains. Moreover, '\n",
            "                                'the summary lengths generated by DeepSeek-R1 '\n",
            "                                'are concise, with an average of 689 tokens on '\n",
            "                                'ArenaHard and 2,218 characters on AlpacaEval '\n",
            "                                '2.0. This indicates that\\n'\n",
            "                                '\\n'\n",
            "                                'DeepSeek-R1 avoids introducing length bias '\n",
            "                                'during GPT-based evaluations, further '\n",
            "                                'solidifying its robustness across multiple '\n",
            "                                'tasks.\\n'\n",
            "                                '\\n'\n",
            "                                'On math tasks, DeepSeek-R1 demonstrates '\n",
            "                                'performance on par with OpenAI-o1-1217, '\n",
            "                                'surpassing other models by a large margin. A '\n",
            "                                'similar trend is observed on coding algorithm '\n",
            "                                'tasks, such as LiveCodeBench and Codeforces, '\n",
            "                                'where reasoning-focused models dominate these '\n",
            "                                'benchmarks. On engineering-oriented coding '\n",
            "                                'tasks, OpenAI-o1-1217 outperforms DeepSeek-R1 '\n",
            "                                'on Aider but achieves comparable performance '\n",
            "                                'on SWE Verified. We believe the engineering '\n",
            "                                'performance of DeepSeek-R1 will improve in '\n",
            "                                'the next version, as the amount of related RL '\n",
            "                                'training data currently remains very '\n",
            "                                'limited.\\n',\n",
            "                        'title': '3.1. DeepSeek-R1 Evaluation'},\n",
            "                       {'node_id': '0022',\n",
            "                        'page_index': 14,\n",
            "                        'text': '### 3.2. Distilled Model Evaluation\\n'\n",
            "                                '\\n'\n",
            "                                '| Model | AIME 2024 |  | MATH-500 | GPQA <br> '\n",
            "                                'Diamond | LiveCode <br> Bench | CodeForces |\\n'\n",
            "                                '| :-- | :--: | :--: | :--: | :--: | :--: | '\n",
            "                                ':--: |\\n'\n",
            "                                '|  | pass@1 | cons@64 | pass@1 | pass@1 | '\n",
            "                                'pass@1 | rating |\\n'\n",
            "                                '| GPT-4o-0513 | 9.3 | 13.4 | 74.6 | 49.9 | '\n",
            "                                '32.9 | 759 |\\n'\n",
            "                                '| Claude-3.5-Sonnet-1022 | 16.0 | 26.7 | 78.3 '\n",
            "                                '| 65.0 | 38.9 | 717 |\\n'\n",
            "                                '| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 '\n",
            "                                '| 53.8 | $\\\\mathbf{1 8 2 0}$ |\\n'\n",
            "                                '| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 '\n",
            "                                '| 41.9 | 1316 |\\n'\n",
            "                                '| DeepSeek-R1-Distill-Qwen-1.5B | 28.9 | 52.7 '\n",
            "                                '| 83.9 | 33.8 | 16.9 | 954 |\\n'\n",
            "                                '| DeepSeek-R1-Distill-Qwen-7B | 55.5 | 83.3 | '\n",
            "                                '92.8 | 49.1 | 37.6 | 1189 |\\n'\n",
            "                                '| DeepSeek-R1-Distill-Qwen-14B | 69.7 | 80.0 '\n",
            "                                '| 93.9 | 59.1 | 53.1 | 1481 |\\n'\n",
            "                                '| DeepSeek-R1-Distill-Qwen-32B | $\\\\mathbf{7 '\n",
            "                                '2 . 6}$ | 83.3 | 94.3 | 62.1 | 57.2 | 1691 |\\n'\n",
            "                                '| DeepSeek-R1-Distill-Llama-8B | 50.4 | 80.0 '\n",
            "                                '| 89.1 | 49.0 | 39.6 | 1205 |\\n'\n",
            "                                '| DeepSeek-R1-Distill-Llama-70B | 70.0 | '\n",
            "                                '$\\\\mathbf{8 6 . 7}$ | $\\\\mathbf{9 4 . 5}$ | '\n",
            "                                '$\\\\mathbf{6 5 . 2}$ | $\\\\mathbf{5 7 . 5}$ | '\n",
            "                                '1633 |\\n'\n",
            "                                '\\n'\n",
            "                                'Table 5 | Comparison of DeepSeek-R1 distilled '\n",
            "                                'models and other comparable models on '\n",
            "                                'reasoning-related benchmarks.\\n'\n",
            "                                '\\n'\n",
            "                                'As shown in Table 5, simply distilling '\n",
            "                                \"DeepSeek-R1's outputs enables the efficient \"\n",
            "                                'DeepSeekR1-7B (i.e., '\n",
            "                                'DeepSeek-R1-Distill-Qwen-7B, abbreviated '\n",
            "                                'similarly below) to outperform nonreasoning '\n",
            "                                'models like GPT-4o-0513 across the board. '\n",
            "                                'DeepSeek-R1-14B surpasses QwQ-32BPreview on '\n",
            "                                'all evaluation metrics, while DeepSeek-R1-32B '\n",
            "                                'and DeepSeek-R1-70B significantly exceed '\n",
            "                                'o1-mini on most benchmarks. These results '\n",
            "                                'demonstrate the strong potential of '\n",
            "                                'distillation. Additionally, we found that '\n",
            "                                'applying RL to these distilled models yields '\n",
            "                                'significant further gains. We believe this '\n",
            "                                'warrants further exploration and therefore '\n",
            "                                'present only the results of the simple '\n",
            "                                'SFT-distilled models here.\\n',\n",
            "                        'title': '3.2. Distilled Model Evaluation'}],\n",
            "             'page_index': 11,\n",
            "             'text': '## 3. Experiment\\n'\n",
            "                     '\\n'\n",
            "                     'Benchmarks We evaluate models on MMLU (Hendrycks et al., '\n",
            "                     '2020), MMLU-Redux (Gema et al., 2024), MMLU-Pro (Wang et '\n",
            "                     'al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li '\n",
            "                     'et al., 2023), IFEval (Zhou et al., 2023), FRAMES '\n",
            "                     '(Krishna et al., 2024), GPQA Diamond (Rein et al., '\n",
            "                     '2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., '\n",
            "                     '2024), SWE-Bench Verified (OpenAI,\\n'\n",
            "                     '\\n'\n",
            "                     '2024d), Aider ${ }^{1}$, LiveCodeBench (Jain et al., '\n",
            "                     '2024) (2024-08 - 2025-01), Codeforces ${ }^{2}$, Chinese '\n",
            "                     'National High School Mathematics Olympiad (CNMO 2024) ${ '\n",
            "                     '}^{3}$, and American Invitational Mathematics '\n",
            "                     'Examination 2024 (AIME 2024) (MAA, 2024). In addition to '\n",
            "                     'standard benchmarks, we also evaluate our models on '\n",
            "                     'open-ended generation tasks using LLMs as judges. '\n",
            "                     'Specifically, we adhere to the original configurations '\n",
            "                     'of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard '\n",
            "                     '(Li et al., 2024), which leverage GPT-4-Turbo-1106 as '\n",
            "                     'judges for pairwise comparisons. Here, we only feed the '\n",
            "                     'final summary to evaluation to avoid the length bias. '\n",
            "                     'For distilled models, we report representative results '\n",
            "                     'on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and '\n",
            "                     'LiveCodeBench.\\n'\n",
            "                     '\\n'\n",
            "                     'Evaluation Prompts Following the setup in DeepSeek-V3, '\n",
            "                     'standard benchmarks such as MMLU, DROP, GPQA Diamond, '\n",
            "                     'and SimpleQA are evaluated using prompts from the '\n",
            "                     'simpleevals framework. For MMLU-Redux, we adopt the '\n",
            "                     'Zero-Eval prompt format (Lin, 2024) in a zero-shot '\n",
            "                     'setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, '\n",
            "                     'since the original prompts are few-shot, we slightly '\n",
            "                     'modify the prompt to the zero-shot setting. The CoT in '\n",
            "                     'few-shot may hurt the performance of DeepSeek-R1. Other '\n",
            "                     'datasets follow their original evaluation protocols with '\n",
            "                     'default prompts provided by their creators. For code and '\n",
            "                     'math benchmarks, the HumanEval-Mul dataset covers eight '\n",
            "                     'mainstream programming languages (Python, Java, C++, '\n",
            "                     'C\\\\#, JavaScript, TypeScript, PHP, and Bash). Model '\n",
            "                     'performance on LiveCodeBench is evaluated using CoT '\n",
            "                     'format, with data collected between August 2024 and '\n",
            "                     'January 2025. The Codeforces dataset is evaluated using '\n",
            "                     'problems from 10 Div. 2 contests along with '\n",
            "                     'expert-crafted test cases, after which the expected '\n",
            "                     'ratings and percentages of competitors are calculated. '\n",
            "                     'SWE-Bench verified results are obtained via the '\n",
            "                     'agentless framework (Xia et al., 2024). AIDER-related '\n",
            "                     'benchmarks are measured using a \"diff\" format. '\n",
            "                     'DeepSeek-R1 outputs are capped at a maximum of 32,768 '\n",
            "                     'tokens for each benchmark.\\n'\n",
            "                     '\\n'\n",
            "                     'Baselines We conduct comprehensive evaluations against '\n",
            "                     'several strong baselines, including DeepSeek-V3, '\n",
            "                     'Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and '\n",
            "                     'OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API '\n",
            "                     'is challenging in mainland China, we report its '\n",
            "                     'performance based on official reports. For distilled '\n",
            "                     'models, we also compare the open-source model '\n",
            "                     'QwQ-32B-Preview (Qwen, 2024a).\\n'\n",
            "                     '\\n'\n",
            "                     'Evaluation Setup We set the maximum generation length to '\n",
            "                     '32,768 tokens for the models. We found that using greedy '\n",
            "                     'decoding to evaluate long-output reasoning models '\n",
            "                     'results in higher repetition rates and significant '\n",
            "                     'variability across different checkpoints. Therefore, we '\n",
            "                     'default to pass@ $k$ evaluation (Chen et al., 2021) and '\n",
            "                     'report pass@1 using a non-zero temperature. '\n",
            "                     'Specifically, we use a sampling temperature of 0.6 and a '\n",
            "                     'top- $p$ value of 0.95 to generate $k$ responses '\n",
            "                     '(typically between 4 and 64, depending on the test set '\n",
            "                     'size) for each question. Pass@1 is then calculated as\\n'\n",
            "                     '\\n'\n",
            "                     '$$\\n'\n",
            "                     '\\\\text { pass@1 }=\\\\frac{1}{k} \\\\sum_{i=1}^{k} p_{i}\\n'\n",
            "                     '$$\\n'\n",
            "                     '\\n'\n",
            "                     'where $p_{i}$ denotes the correctness of the $i$-th '\n",
            "                     'response. This method provides more reliable performance '\n",
            "                     'estimates. For AIME 2024, we also report consensus '\n",
            "                     '(majority vote) results (Wang et al., 2022) using 64 '\n",
            "                     'samples, denoted as cons@64.\\n'\n",
            "                     '\\n'\n",
            "                     '[^0]\\n'\n",
            "                     '[^0]:    ${ }^{1}$ https://aider.chat\\n'\n",
            "                     '    ${ }^{2}$ https://codeforces.com\\n'\n",
            "                     '    ${ }^{3}$ '\n",
            "                     'https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n',\n",
            "             'title': '3. Experiment'},\n",
            "            {'node_id': '0023',\n",
            "             'nodes': [{'node_id': '0024',\n",
            "                        'page_index': 14,\n",
            "                        'text': '### 4.1. Distillation v.s. Reinforcement '\n",
            "                                'Learning\\n'\n",
            "                                '\\n'\n",
            "                                'In Section 3.2, we can see that by distilling '\n",
            "                                'DeepSeek-R1, the small model can achieve '\n",
            "                                'impressive results. However, there is still '\n",
            "                                'one question left: can the model achieve '\n",
            "                                'comparable performance through the '\n",
            "                                'large-scale RL training discussed in the '\n",
            "                                'paper without distillation?\\n'\n",
            "                                '\\n'\n",
            "                                'To answer this question, we conduct '\n",
            "                                'large-scale RL training on Qwen-32B-Base '\n",
            "                                'using math, code, and STEM data, training for '\n",
            "                                'over 10K steps, resulting in '\n",
            "                                'DeepSeek-R1-Zero-Qwen-32B. The experimental '\n",
            "                                'results, shown in Table 6, demonstrate that '\n",
            "                                'the 32B base model, after large-scale\\n'\n",
            "                                '\\n'\n",
            "                                '| Model | AIME 2024 |  | MATH-500 | GPQA '\n",
            "                                'Diamond | LiveCodeBench |\\n'\n",
            "                                '| :-- | :--: | :--: | :--: | :--: | :--: |\\n'\n",
            "                                '|  | pass@1 | cons@64 | pass@1 | pass@1 | '\n",
            "                                'pass@1 |\\n'\n",
            "                                '| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 '\n",
            "                                '| 41.9 |\\n'\n",
            "                                '| DeepSeek-R1-Zero-Qwen-32B | 47.0 | 60.0 | '\n",
            "                                '91.6 | 55.0 | 40.2 |\\n'\n",
            "                                '| DeepSeek-R1-Distill-Qwen-32B | $\\\\mathbf{7 '\n",
            "                                '2 . 6}$ | $\\\\mathbf{8 3 . 3}$ | $\\\\mathbf{9 4 '\n",
            "                                '. 3}$ | $\\\\mathbf{6 2 . 1}$ | $\\\\mathbf{5 7 . '\n",
            "                                '2}$ |\\n'\n",
            "                                '\\n'\n",
            "                                'Table 6 | Comparison of distilled and RL '\n",
            "                                'Models on Reasoning-Related Benchmarks.\\n'\n",
            "                                '\\n'\n",
            "                                'RL training, achieves performance on par with '\n",
            "                                'QwQ-32B-Preview. However, '\n",
            "                                'DeepSeek-R1-Distill-Qwen-32B, which is '\n",
            "                                'distilled from DeepSeek-R1, performs '\n",
            "                                'significantly better than '\n",
            "                                'DeepSeek-R1-Zero-Qwen-32B across all '\n",
            "                                'benchmarks.\\n'\n",
            "                                '\\n'\n",
            "                                'Therefore, we can draw two conclusions: '\n",
            "                                'First, distilling more powerful models into '\n",
            "                                'smaller ones yields excellent results, '\n",
            "                                'whereas smaller models relying on the '\n",
            "                                'large-scale RL mentioned in this paper '\n",
            "                                'require enormous computational power and may '\n",
            "                                'not even achieve the performance of '\n",
            "                                'distillation. Second, while distillation '\n",
            "                                'strategies are both economical and effective, '\n",
            "                                'advancing beyond the boundaries of '\n",
            "                                'intelligence may still require more powerful '\n",
            "                                'base models and largerscale reinforcement '\n",
            "                                'learning.\\n',\n",
            "                        'title': '4.1. Distillation v.s. Reinforcement '\n",
            "                                 'Learning'},\n",
            "                       {'node_id': '0025',\n",
            "                        'page_index': 15,\n",
            "                        'text': '### 4.2. Unsuccessful Attempts\\n'\n",
            "                                '\\n'\n",
            "                                'In the early stages of developing '\n",
            "                                'DeepSeek-R1, we also encountered failures and '\n",
            "                                'setbacks along the way. We share our failure '\n",
            "                                'experiences here to provide insights, but '\n",
            "                                'this does not imply that these approaches are '\n",
            "                                'incapable of developing effective reasoning '\n",
            "                                'models.\\n'\n",
            "                                '\\n'\n",
            "                                'Process Reward Model (PRM) PRM is a '\n",
            "                                'reasonable method to guide the model toward '\n",
            "                                'better approaches for solving reasoning tasks '\n",
            "                                '(Lightman et al., 2023; Uesato et al., 2022; '\n",
            "                                'Wang et al., 2023). However, in practice, PRM '\n",
            "                                'has three main limitations that may hinder '\n",
            "                                'its ultimate success. First, it is '\n",
            "                                'challenging to explicitly define a fine-grain '\n",
            "                                'step in general reasoning. Second, '\n",
            "                                'determining whether the current intermediate '\n",
            "                                'step is correct is a challenging task. '\n",
            "                                'Automated annotation using models may not '\n",
            "                                'yield satisfactory results, while manual '\n",
            "                                'annotation is not conducive to scaling up. '\n",
            "                                'Third, once a model-based PRM is introduced, '\n",
            "                                'it inevitably leads to reward hacking (Gao et '\n",
            "                                'al., 2022), and retraining the reward model '\n",
            "                                'needs additional training resources and it '\n",
            "                                'complicates the whole training pipeline. In '\n",
            "                                'conclusion, while PRM demonstrates a good '\n",
            "                                'ability to rerank the top-N responses '\n",
            "                                'generated by the model or assist in guided '\n",
            "                                'search (Snell et al., 2024), its advantages '\n",
            "                                'are limited compared to the additional '\n",
            "                                'computational overhead it introduces during '\n",
            "                                'the large-scale reinforcement learning '\n",
            "                                'process in our experiments.\\n'\n",
            "                                '\\n'\n",
            "                                'Monte Carlo Tree Search (MCTS) Inspired by '\n",
            "                                'AlphaGo (Silver et al., 2017b) and AlphaZero '\n",
            "                                '(Silver et al., 2017a), we explored using '\n",
            "                                'Monte Carlo Tree Search (MCTS) to enhance '\n",
            "                                'test-time compute scalability. This approach '\n",
            "                                'involves breaking answers into smaller parts '\n",
            "                                'to allow the model to explore the solution '\n",
            "                                'space systematically. To facilitate this, we '\n",
            "                                'prompt the model to generate multiple tags '\n",
            "                                'that correspond to specific reasoning steps '\n",
            "                                'necessary for the search. For training, we '\n",
            "                                'first use collected prompts to find answers '\n",
            "                                'via MCTS guided by a pre-trained value model. '\n",
            "                                'Subsequently, we use the resulting '\n",
            "                                'question-answer pairs to train both the actor '\n",
            "                                'model and the value model, iteratively '\n",
            "                                'refining the process.\\n'\n",
            "                                '\\n'\n",
            "                                'However, this approach encounters several '\n",
            "                                'challenges when scaling up the training. '\n",
            "                                'First, unlike chess, where the search space '\n",
            "                                'is relatively well-defined, token generation '\n",
            "                                'presents an\\n'\n",
            "                                '\\n'\n",
            "                                'exponentially larger search space. To address '\n",
            "                                'this, we set a maximum extension limit for '\n",
            "                                'each node, but this can lead to the model '\n",
            "                                'getting stuck in local optima. Second, the '\n",
            "                                'value model directly influences the quality '\n",
            "                                'of generation since it guides each step of '\n",
            "                                'the search process. Training a fine-grained '\n",
            "                                'value model is inherently difficult, which '\n",
            "                                'makes it challenging for the model to '\n",
            "                                \"iteratively improve. While AlphaGo's core \"\n",
            "                                'success relied on training a value model to '\n",
            "                                'progressively enhance its performance, this '\n",
            "                                'principle proves difficult to replicate in '\n",
            "                                'our setup due to the complexities of token '\n",
            "                                'generation.\\n'\n",
            "                                '\\n'\n",
            "                                'In conclusion, while MCTS can improve '\n",
            "                                'performance during inference when paired with '\n",
            "                                'a pre-trained value model, iteratively '\n",
            "                                'boosting model performance through '\n",
            "                                'self-search remains a significant '\n",
            "                                'challenge.\\n',\n",
            "                        'title': '4.2. Unsuccessful Attempts'}],\n",
            "             'page_index': 14,\n",
            "             'text': '## 4. Discussion\\n',\n",
            "             'title': '4. Discussion'},\n",
            "            {'node_id': '0026',\n",
            "             'page_index': 16,\n",
            "             'text': '## 5. Conclusion, Limitations, and Future Work\\n'\n",
            "                     '\\n'\n",
            "                     'In this work, we share our journey in enhancing model '\n",
            "                     'reasoning abilities through reinforcement learning. '\n",
            "                     'DeepSeek-R1-Zero represents a pure RL approach without '\n",
            "                     'relying on cold-start data, achieving strong performance '\n",
            "                     'across various tasks. DeepSeek-R1 is more powerful, '\n",
            "                     'leveraging cold-start data alongside iterative RL '\n",
            "                     'fine-tuning. Ultimately, DeepSeek-R1 achieves '\n",
            "                     'performance comparable to OpenAI-o1-1217 on a range of '\n",
            "                     'tasks.\\n'\n",
            "                     '\\n'\n",
            "                     'We further explore distillation the reasoning capability '\n",
            "                     'to small dense models. We use DeepSeek-R1 as the teacher '\n",
            "                     'model to generate 800K training samples, and fine-tune '\n",
            "                     'several small dense models. The results are promising: '\n",
            "                     'DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and '\n",
            "                     'Claude-3.5-Sonnet on math benchmarks with $28.9 \\\\%$ on '\n",
            "                     'AIME and $83.9 \\\\%$ on MATH. Other dense models also '\n",
            "                     'achieve impressive results, significantly outperforming '\n",
            "                     'other instructiontuned models based on the same '\n",
            "                     'underlying checkpoints.\\n'\n",
            "                     '\\n'\n",
            "                     'In the future, we plan to invest in research across the '\n",
            "                     'following directions for DeepSeek-R1.\\n'\n",
            "                     '\\n'\n",
            "                     '- General Capability: Currently, the capabilities of '\n",
            "                     'DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as '\n",
            "                     'function calling, multi-turn, complex role-playing, and '\n",
            "                     'JSON output. Moving forward, we plan to explore how long '\n",
            "                     'CoT can be leveraged to enhance tasks in these fields.\\n'\n",
            "                     '- Language Mixing: DeepSeek-R1 is currently optimized '\n",
            "                     'for Chinese and English, which may result in language '\n",
            "                     'mixing issues when handling queries in other languages. '\n",
            "                     'For instance, DeepSeek-R1 might use English for '\n",
            "                     'reasoning and responses, even if the query is in a '\n",
            "                     'language other than English or Chinese. We aim to '\n",
            "                     'address this limitation in future updates.\\n'\n",
            "                     '- Prompting Engineering: When evaluating DeepSeek-R1, we '\n",
            "                     'observe that it is sensitive to prompts. Few-shot '\n",
            "                     'prompting consistently degrades its performance. '\n",
            "                     'Therefore, we recommend users directly describe the '\n",
            "                     'problem and specify the output format using a zero-shot '\n",
            "                     'setting for optimal results.\\n'\n",
            "                     '- Software Engineering Tasks: Due to the long evaluation '\n",
            "                     'times, which impact the efficiency of the RL process, '\n",
            "                     'large-scale RL has not been applied extensively in '\n",
            "                     'software engineering tasks. As a result, DeepSeek-R1 has '\n",
            "                     'not demonstrated a huge improvement over DeepSeek-V3 on '\n",
            "                     'software engineering benchmarks. Future versions will '\n",
            "                     'address this by implementing rejection sampling on '\n",
            "                     'software engineering data or incorporating asynchronous '\n",
            "                     'evaluations during the RL process to improve '\n",
            "                     'efficiency.\\n',\n",
            "             'title': '5. Conclusion, Limitations, and Future Work'},\n",
            "            {'node_id': '0027',\n",
            "             'page_index': 17,\n",
            "             'text': '## References\\n'\n",
            "                     '\\n'\n",
            "                     'AI@Meta. Llama 3.1 model card, 2024. URL '\n",
            "                     'https://github.com/meta-llama/llama-m '\n",
            "                     'odels/blob/main/models/llama3_1/MODEL_CARD.md.\\n'\n",
            "                     '\\n'\n",
            "                     'Anthropic. Claude 3.5 sonnet, 2024. URL '\n",
            "                     'https://www.anthropic.com/news/claude-3 -5-sonnet.\\n'\n",
            "                     'M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira '\n",
            "                     'Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. '\n",
            "                     'Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. '\n",
            "                     'Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. '\n",
            "                     'Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. '\n",
            "                     'Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, '\n",
            "                     'F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. '\n",
            "                     'Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. '\n",
            "                     'Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. '\n",
            "                     'Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. '\n",
            "                     'Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, '\n",
            "                     'B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and '\n",
            "                     'W. Zaremba. Evaluating large language models trained on '\n",
            "                     'code. CoRR, abs/2107.03374, 2021. URL '\n",
            "                     'https://arxiv.org/abs/2107.03374.\\n'\n",
            "                     'A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, '\n",
            "                     'A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et '\n",
            "                     'al. The llama 3 herd of models. arXiv preprint '\n",
            "                     'arXiv:2407.21783, 2024.\\n'\n",
            "                     'Y. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. '\n",
            "                     'Length-controlled alpacaeval: A simple way to debias '\n",
            "                     'automatic evaluators. arXiv preprint arXiv:2404.04475, '\n",
            "                     '2024.\\n'\n",
            "                     'X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. '\n",
            "                     'Zhang, and J. Wang. Alphazero-like tree-search can guide '\n",
            "                     'large language model decoding and training, 2024. URL '\n",
            "                     'https: //arxiv.org/abs/2309.17179.\\n'\n",
            "                     'L. Gao, J. Schulman, and J. Hilton. Scaling laws for '\n",
            "                     'reward model overoptimization, 2022. URL '\n",
            "                     'https://arxiv.org/abs/2210.10760.\\n'\n",
            "                     'A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. '\n",
            "                     'Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. '\n",
            "                     'Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. '\n",
            "                     'van Krieken, and P. Minervini. Are we done with mmlu? '\n",
            "                     'CoRR, abs/2406.04127, 2024. URL https://doi . or '\n",
            "                     'g/10.48550/arXiv. 2406.04127.\\n'\n",
            "                     '\\n'\n",
            "                     'Google. Our next-generation model: Gemini 1.5, 2024. URL '\n",
            "                     'https://blog.google/techno '\n",
            "                     'logy/ai/google-gemini-next-generation-model-february-2024.\\n'\n",
            "                     'Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, '\n",
            "                     'H. Guo, C. Hu, B. Zheng, et al. Chinese simpleqa: A '\n",
            "                     'chinese factuality evaluation for large language models. '\n",
            "                     'arXiv preprint arXiv:2411.07140, 2024.\\n'\n",
            "                     'D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, '\n",
            "                     'D. Song, and J. Steinhardt. Measuring massive multitask '\n",
            "                     'language understanding. arXiv preprint arXiv:2009.03300, '\n",
            "                     '2020.\\n'\n",
            "                     'Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. '\n",
            "                     'Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A '\n",
            "                     'multi-level multi-discipline chinese evaluation suite '\n",
            "                     'for foundation models. arXiv preprint arXiv:2305.08322, '\n",
            "                     '2023.\\n'\n",
            "                     'N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. '\n",
            "                     'Wang, A. Solar-Lezama, K. Sen, and I. Stoica. '\n",
            "                     'Livecodebench: Holistic and contamination free '\n",
            "                     'evaluation of large language models for code. CoRR, '\n",
            "                     'abs/2403.07974, 2024. URL '\n",
            "                     'https://doi.org/10.48550/arXiv.2403.07974.\\n'\n",
            "                     '\\n'\n",
            "                     'S. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. '\n",
            "                     'Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and '\n",
            "                     'reason: A unified evaluation of retrieval-augmented '\n",
            "                     'generation. CoRR, abs/2409.12941, 2024. doi: '\n",
            "                     '10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 '\n",
            "                     '50/arXiv. 2409.12941.\\n'\n",
            "                     'A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, '\n",
            "                     'A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et '\n",
            "                     'al. Training language models to self-correct via '\n",
            "                     'reinforcement learning. arXiv preprint arXiv:2409.12917, '\n",
            "                     '2024.\\n'\n",
            "                     'H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. '\n",
            "                     'Duan, and T. Baldwin. CMMLU: Measuring massive multitask '\n",
            "                     'language understanding in Chinese. arXiv preprint '\n",
            "                     'arXiv:2306.09212, 2023.\\n'\n",
            "                     'T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, '\n",
            "                     'J. E. Gonzalez, and I. Stoica. From crowdsourced data to '\n",
            "                     'high-quality benchmarks: Arena-hard and benchbuilder '\n",
            "                     'pipeline. arXiv preprint arXiv:2406.11939, 2024.\\n'\n",
            "                     'H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. '\n",
            "                     'Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and '\n",
            "                     \"K. Cobbe. Let's verify step by step. arXiv preprint \"\n",
            "                     'arXiv:2305.20050, 2023.\\n'\n",
            "                     'B. Y. Lin. ZeroEval: A Unified Framework for Evaluating '\n",
            "                     'Language Models, July 2024. URL '\n",
            "                     'https://github.com/WildEval/ZeroEval.\\n'\n",
            "                     '\\n'\n",
            "                     'MAA. American invitational mathematics examination - '\n",
            "                     'aime. In American Invitational Mathematics Examination - '\n",
            "                     'AIME 2024, February 2024. URL https://maa.org/math '\n",
            "                     '-competitions/american-invitational-mathematics-examination-aime.\\n'\n",
            "                     '\\n'\n",
            "                     'OpenAI. Hello GPT-4o, 2024a. URL '\n",
            "                     'https://openai.com/index/hello-gpt-4o/.\\n'\n",
            "                     'OpenAI. Learning to reason with llms, 2024b. URL '\n",
            "                     'https://openai.com/index/learnin '\n",
            "                     'g-to-reason-with-llms/.\\n'\n",
            "                     '\\n'\n",
            "                     'OpenAI. Introducing SimpleQA, 2024c. URL '\n",
            "                     'https://openai.com/index/introducing -simpleqa/.\\n'\n",
            "                     '\\n'\n",
            "                     \"OpenAI. Introducing SWE-bench verified we're releasing a \"\n",
            "                     'human-validated subset of swebench that more, 2024d. URL '\n",
            "                     'https://openai.com/index/introducing-swe-bench '\n",
            "                     '-verified/.\\n'\n",
            "                     '\\n'\n",
            "                     'Qwen. Qwq: Reflect deeply on the boundaries of the '\n",
            "                     'unknown, 2024a. URL https://qwenlm '\n",
            "                     '.github.io/blog/qwq-32b-preview/.\\n'\n",
            "                     '\\n'\n",
            "                     'Qwen. Qwen2.5: A party of foundation models, 2024b. URL '\n",
            "                     'https://qwenlm.github.io/b log/qwen2.5.\\n'\n",
            "                     'D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. '\n",
            "                     'Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A '\n",
            "                     'graduate-level google-proof q\\\\&a benchmark. arXiv '\n",
            "                     'preprint arXiv:2311.12022, 2023.\\n'\n",
            "                     'Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. '\n",
            "                     'Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits '\n",
            "                     'of mathematical reasoning in open language models. arXiv '\n",
            "                     'preprint arXiv:2402.03300, 2024.\\n'\n",
            "                     'D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, '\n",
            "                     'M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. '\n",
            "                     'Graepel, T. P. Lillicrap, K. Simonyan, and D. Hassabis. '\n",
            "                     'Mastering chess and shogi by self-play with a general '\n",
            "                     'reinforcement learning algorithm. CoRR, abs/1712.01815, '\n",
            "                     '2017a. URL http://arxiv.org/abs/1712.01815.\\n'\n",
            "                     '\\n'\n",
            "                     'D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, '\n",
            "                     'A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. '\n",
            "                     'Bolton, Y. Chen, T. P. Lillicrap, F. Hui, L. Sifre, G. '\n",
            "                     'van den Driessche, T. Graepel, and D. Hassabis. '\n",
            "                     'Mastering the game of go without human knowledge. Nat., '\n",
            "                     '550(7676):354-359, 2017b. doi: 10.1038/NATURE24270. URL '\n",
            "                     'https://doi.org/10.1038/nature24270.\\n'\n",
            "                     'C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm '\n",
            "                     'test-time compute optimally can be more effective than '\n",
            "                     'scaling model parameters, 2024. URL '\n",
            "                     'https://arxiv.org/abs/2408.033 14 .\\n'\n",
            "                     'T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving '\n",
            "                     'olympiad geometry without human demonstrations. Nature, '\n",
            "                     '2024. doi: 10.1038/s41586-023-06747-5.\\n'\n",
            "                     'J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. '\n",
            "                     'Wang, A. Creswell, G. Irving, and I. Higgins. Solving '\n",
            "                     'math word problems with process-and outcome-based '\n",
            "                     'feedback. arXiv preprint arXiv:2211.14275, 2022.\\n'\n",
            "                     'P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, '\n",
            "                     'Y. Wu, and Z. Sui. Math-shepherd: A labelfree '\n",
            "                     'step-by-step verifier for llms in mathematical '\n",
            "                     'reasoning. arXiv preprint arXiv:2312.08935, 2023.\\n'\n",
            "                     'X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. '\n",
            "                     'Narang, A. Chowdhery, and D. Zhou. Self-consistency '\n",
            "                     'improves chain of thought reasoning in language models. '\n",
            "                     'arXiv preprint arXiv:2203.11171, 2022.\\n'\n",
            "                     'Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. '\n",
            "                     'Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, '\n",
            "                     'A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more '\n",
            "                     'robust and challenging multi-task language understanding '\n",
            "                     'benchmark. CoRR, abs/2406.01574, 2024. URL '\n",
            "                     'https://doi.org/10.48550/arXiv.2406.01574.\\n'\n",
            "                     'C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: '\n",
            "                     'Demystifying llm-based software engineering agents. '\n",
            "                     'arXiv preprint, 2024.\\n'\n",
            "                     'H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, '\n",
            "                     'B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, '\n",
            "                     'Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. '\n",
            "                     'Deepseek-prover-v1.5: Harnessing proof assistant '\n",
            "                     'feedback for reinforcement learning and monte-carlo tree '\n",
            "                     'search, 2024. URL https://arxiv.org/abs/2408.08152.\\n'\n",
            "                     'J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, '\n",
            "                     'D. Zhou, and L. Hou. Instruction-following evaluation '\n",
            "                     'for large language models. arXiv preprint '\n",
            "                     'arXiv:2311.07911, 2023.\\n',\n",
            "             'title': 'References'},\n",
            "            {'node_id': '0028',\n",
            "             'page_index': 20,\n",
            "             'text': '## Appendix\\n',\n",
            "             'title': 'Appendix'},\n",
            "            {'node_id': '0029',\n",
            "             'nodes': [{'node_id': '0030',\n",
            "                        'page_index': 20,\n",
            "                        'text': '### Core Contributors\\n'\n",
            "                                '\\n'\n",
            "                                'Daya Guo\\n'\n",
            "                                'Dejian Yang\\n'\n",
            "                                'Haowei Zhang\\n'\n",
            "                                'Junxiao Song\\n'\n",
            "                                'Ruoyu Zhang\\n'\n",
            "                                'Runxin Xu\\n'\n",
            "                                'Qihao Zhu\\n'\n",
            "                                'Shirong Ma\\n'\n",
            "                                'Peiyi Wang\\n'\n",
            "                                'Xiao Bi\\n'\n",
            "                                'Xiaokang Zhang\\n'\n",
            "                                'Xingkai Yu\\n'\n",
            "                                'Yu Wu\\n'\n",
            "                                'Z.F. Wu\\n'\n",
            "                                '\\n'\n",
            "                                'Zhibin Gou\\n'\n",
            "                                'Zhihong Shao\\n'\n",
            "                                'Zhuoshu Li\\n'\n",
            "                                'Ziyi Gao\\n',\n",
            "                        'title': 'Core Contributors'},\n",
            "                       {'node_id': '0031',\n",
            "                        'page_index': 20,\n",
            "                        'text': '### Contributors\\n'\n",
            "                                '\\n'\n",
            "                                'Aixin Liu\\n'\n",
            "                                'Bing Xue\\n'\n",
            "                                'Bingxuan Wang\\n'\n",
            "                                'Bochao Wu\\n'\n",
            "                                'Bei Feng\\n'\n",
            "                                'Chengda Lu\\n'\n",
            "                                'Chenggang Zhao\\n'\n",
            "                                'Chengqi Deng\\n'\n",
            "                                'Chong Ruan\\n'\n",
            "                                'Damai Dai\\n'\n",
            "                                'Deli Chen\\n'\n",
            "                                'Dongjie Ji\\n'\n",
            "                                'Erhang Li\\n'\n",
            "                                'Fangyun Lin\\n'\n",
            "                                'Fucong Dai\\n'\n",
            "                                'Fuli Luo*\\n'\n",
            "                                'Guangbo Hao\\n'\n",
            "                                'Guanting Chen\\n'\n",
            "                                'Guowei Li\\n'\n",
            "                                'H. Zhang\\n'\n",
            "                                '\\n'\n",
            "                                'Hanwei Xu\\n'\n",
            "                                'Honghui Ding\\n'\n",
            "                                'Huazuo Gao\\n'\n",
            "                                'Hui Qu\\n'\n",
            "                                '\\n'\n",
            "                                'Hui Li\\n'\n",
            "                                'Jianzhong Guo\\n'\n",
            "                                'Jiashi Li\\n'\n",
            "                                'Jingchang Chen\\n'\n",
            "                                'Jingyang Yuan\\n'\n",
            "                                'Jinhao Tu\\n'\n",
            "                                'Junjie Qiu\\n'\n",
            "                                'Junlong Li\\n'\n",
            "                                'J.L. Cai\\n'\n",
            "                                '\\n'\n",
            "                                'Jiaqi Ni\\n'\n",
            "                                'Jian Liang\\n'\n",
            "                                'Jin Chen\\n'\n",
            "                                'Kai Dong\\n'\n",
            "                                'Kai Hu*\\n'\n",
            "                                'Kaichao You\\n'\n",
            "                                'Kaige Gao\\n'\n",
            "                                'Kang Guan\\n'\n",
            "                                'Kexin Huang\\n'\n",
            "                                'Kuai Yu\\n'\n",
            "                                'Lean Wang\\n'\n",
            "                                'Lecong Zhang\\n'\n",
            "                                'Liang Zhao\\n'\n",
            "                                'Litong Wang\\n'\n",
            "                                'Liyue Zhang\\n'\n",
            "                                'Lei Xu\\n'\n",
            "                                'Leyi Xia\\n'\n",
            "                                'Mingchuan Zhang\\n'\n",
            "                                'Minghua Zhang\\n'\n",
            "                                'Minghui Tang\\n'\n",
            "                                'Mingxu Zhou\\n'\n",
            "                                'Meng Li\\n'\n",
            "                                'Miaojun Wang\\n'\n",
            "                                'Mingming Li\\n'\n",
            "                                'Ning Tian\\n'\n",
            "                                'Panpan Huang\\n'\n",
            "                                'Peng Zhang\\n'\n",
            "                                'Qiancheng Wang\\n'\n",
            "                                'Qinyu Chen\\n'\n",
            "                                'Qiushi Du\\n'\n",
            "                                'Ruiqi Ge*\\n'\n",
            "                                'Ruisong Zhang\\n'\n",
            "                                'Ruizhe Pan\\n'\n",
            "                                'Runji Wang\\n'\n",
            "                                'R.J. Chen\\n'\n",
            "                                'R.L. Jin\\n'\n",
            "                                '\\n'\n",
            "                                '| Ruyi Chen | Y.X. Wei |\\n'\n",
            "                                '| :--: | :--: |\\n'\n",
            "                                '| Shanghao Lu | Yang Zhang |\\n'\n",
            "                                '| Shangyan Zhou | Yanhong Xu |\\n'\n",
            "                                '| Shanhuang Chen | Yao Li |\\n'\n",
            "                                '| Shengfeng Ye | Yao Zhao |\\n'\n",
            "                                '| Shiyu Wang | Yaofeng Sun |\\n'\n",
            "                                '| Shuiping Yu | Yaohui Wang |\\n'\n",
            "                                '| Shunfeng Zhou | Yi Yu |\\n'\n",
            "                                '| Shuting Pan | Yichao Zhang |\\n'\n",
            "                                '| S.S. Li | Yifan Shi |\\n'\n",
            "                                '| Shuang Zhou | Yiliang Xiong |\\n'\n",
            "                                '| Shaoqing Wu | Ying He |\\n'\n",
            "                                '| Shengfeng Ye | Yishi Piao |\\n'\n",
            "                                '| Tao Yun | Yisong Wang |\\n'\n",
            "                                '| Tian Pei | Yixuan Tan |\\n'\n",
            "                                '| Tianyu Sun | Yiyang Ma* |\\n'\n",
            "                                '| T. Wang | Yiyuan Liu |\\n'\n",
            "                                '| Wangding Zeng | Yongqiang Guo |\\n'\n",
            "                                '| Wen Liu | Yuan Ou |\\n'\n",
            "                                '| Wenfeng Liang | Yuduan Wang |\\n'\n",
            "                                '| Wenjun Gao | Yue Gong |\\n'\n",
            "                                '| Wenqin Yu* | Yuheng Zou |\\n'\n",
            "                                '| Wentao Zhang | Yujia He |\\n'\n",
            "                                '| W.L. Xiao | Yunfan Xiong |\\n'\n",
            "                                '| Wei An | Yuxiang Luo |\\n'\n",
            "                                '| Xiaodong Liu | Yuxiang You |\\n'\n",
            "                                '| Xiaohan Wang | Yuxuan Liu |\\n'\n",
            "                                '| Xiaokang Chen | Yuyang Zhou |\\n'\n",
            "                                '| Xiaotao Nie | Y.X. Zhu |\\n'\n",
            "                                '| Xin Cheng | Yanping Huang |\\n'\n",
            "                                '| Xin Liu | Yaohui Li |\\n'\n",
            "                                '| Xin Xie | Yi Zheng |\\n'\n",
            "                                '| Xingchao Liu | Yuchen Zhu |\\n'\n",
            "                                '| Xinyu Yang | Yunxian Ma |\\n'\n",
            "                                '| Xinyuan Li | Ying Tang |\\n'\n",
            "                                '| Xuecheng Su | Yukun Zha |\\n'\n",
            "                                '| Xuheng Lin | Yuting Yan |\\n'\n",
            "                                '| X.Q. Li | Z.Z. Ren |\\n'\n",
            "                                '| Xiangyue Jin | Zehui Ren |\\n'\n",
            "                                '| Xiaojin Shen | Zhangli Sha |\\n'\n",
            "                                '| Xiaosha Chen | Zhe Fu |\\n'\n",
            "                                '| Xiaowen Sun | Zhean Xu |\\n'\n",
            "                                '| Xiaoxiang Wang | Zhenda Xie |\\n'\n",
            "                                '| Xinnan Song | Zhengyan Zhang |\\n'\n",
            "                                '| Xinyi Zhou | Zhewen Hao |\\n'\n",
            "                                '| Xianzu Wang | Zhicheng Ma |\\n'\n",
            "                                '| Xinxia Shan | Zhigang Yan |\\n'\n",
            "                                '| Y.K. Li | Zhiyu Wu |\\n'\n",
            "                                '| Y.Q. Wang | Zihui Gu |\\n'\n",
            "                                '\\n'\n",
            "                                '| Zijia Zhu | Zhen Huang |\\n'\n",
            "                                '| :-- | :-- |\\n'\n",
            "                                '| Zijun Liu* | Zhipeng Xu |\\n'\n",
            "                                '| Zilin Li | Zhongyu Zhang |\\n'\n",
            "                                '| Ziwei Xie | Zhen Zhang |\\n'\n",
            "                                '| Ziyang Song |  |\\n'\n",
            "                                '| Zizheng Pan |  |\\n'\n",
            "                                '\\n'\n",
            "                                'Within each role, authors are listed '\n",
            "                                'alphabetically by the first name. Names '\n",
            "                                'marked with * denote individuals who have '\n",
            "                                'departed from our team.\\n',\n",
            "                        'title': 'Contributors'}],\n",
            "             'page_index': 20,\n",
            "             'text': '## A. Contributions and Acknowledgments\\n',\n",
            "             'title': 'A. Contributions and Acknowledgments'}],\n",
            "  'page_index': 1,\n",
            "  'text': '# DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via '\n",
            "          'Reinforcement Learning\\n'\n",
            "          '\\n'\n",
            "          'DeepSeek-AI<br>research@deepseek.com\\n',\n",
            "  'title': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via '\n",
            "           'Reinforcement Learning'}]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "tree_result = pi_client.get_tree(doc_id)\n",
        "if tree_result.get(\"status\") == \"completed\":\n",
        "    print(\"Document tree structure loaded!\")\n",
        "    pprint(tree_result.get(\"result\"))\n",
        "else:\n",
        "    print(f\"Tree status: {tree_result.get('status')}. Try again later if still processing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRmOOrTBoox1"
      },
      "source": [
        "## 6Ô∏è‚É£ Delete the Document (Cleanup)\n",
        "\n",
        "If you do not need the document any more, you can delete it by running the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fbSbEwWUoox1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document deleted successfully.\n"
          ]
        }
      ],
      "source": [
        "pi_client.delete_document(doc_id)\n",
        "print(\"Document deleted successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QAzzxjooox1"
      },
      "source": [
        "---\n",
        "\n",
        "# üí¨ Notes & Support\n",
        "\n",
        "- Only **PDF files** are supported at this time.\n",
        "- If you have any questions or need help:\n",
        "    - ü§ù [Join the PageIndex Discord](https://discord.gg/VuXuf29EUj)\n",
        "    - üì® [Contact support via Typeform](https://ii2abc2jejf.typeform.com/to/meB40zV0)\n",
        "\n",
        "---\n",
        "\n",
        "### Full SDK Reference  \n",
        "See: [PageIndex OCR SDK Reference](https://pageindex.ai/ocr/sdk) for advanced usage and all available parameters."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
